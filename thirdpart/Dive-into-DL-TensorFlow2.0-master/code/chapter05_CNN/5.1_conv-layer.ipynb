{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二维卷积层\n",
    "\n",
    "卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。本章中介绍的卷积神经网络均使用最常见的二维卷积层。它有高和宽两个空间维度，常用来处理图像数据。本节中，我们将介绍简单形式的二维卷积层的工作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 two dimentional cross-correlation\n",
    "\n",
    "虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。\n",
    "我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为$3 \\times 3$或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即$2 \\times 2$。图5.1中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$。\n",
    "\n",
    "![二维互相关运算](../img/correlation.svg)\n",
    "\n",
    "在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\\\\\n",
    "$$\n",
    "\n",
    "下面我们将上述过程实现在`corr2d`函数里。它接受输入数组`X`与核数组`K`，并输出数组`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w +1)))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j].assign(tf.cast(tf.reduce_sum(X[i:i+h, j:j+w] * K), dtype=tf.float32))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以构造图5.1中的输入数组`X`、核数组`K`来验证二维互相关运算的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[19., 25.],\n",
       "       [37., 43.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[0,1,2], [3,4,5], [6,7,8]])\n",
    "K = tf.constant([[0,1], [2,3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Conv2d\n",
    "\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n",
    "\n",
    "下面基于`corr2d`函数来实现一个自定义的二维卷积层。在构造函数`__init__`里我们声明`weight`和`bias`这两个模型参数。前向计算函数`forward`则是直接调用`corr2d`函数再加上偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, kernel_size):\n",
    "        self.w = self.add_weight(name='w',\n",
    "                                shape=kernel_size,\n",
    "                                initializer=tf.random_normal_initializer())\n",
    "        self.b = self.add_weight(name='b',\n",
    "                                shape=(1,),\n",
    "                                initializer=tf.random_normal_initializer())\n",
    "    def call(self, inputs):\n",
    "        return corr2d(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积窗口形状为$p \\times q$的卷积层称为$p \\times q$卷积层。同样，$p \\times q$卷积或$p \\times q$卷积核说明卷积核的高和宽分别为$p$和$q$。\n",
    "\n",
    "## 5.1.3 edge detection\n",
    "\n",
    "下面我们来看一个卷积层的简单应用：检测图像中物体的边缘，即找到像素变化的位置。首先我们构造一张$6\\times 8$的图像（即高和宽分别为6像素和8像素的图像）。它中间4列为黑（0），其余为白（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 8) dtype=float32, numpy=\n",
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable(tf.ones((6,8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:,2:6].shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们构造一个高和宽分别为1和2的卷积核`K`。当它与输入做互相关运算时，如果横向相邻元素相同，输出为0；否则输出为非0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.constant([[1,-1]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将输入`X`和我们设计的卷积核`K`做互相关运算。可以看出，我们将从白到黑的边缘和从黑到白的边缘分别检测成了1和-1。其余部分的输出全是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 7) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此，我们可以看出，卷积层可通过重复使用卷积核有效地表征局部空间。\n",
    "\n",
    "## 5.1.4 learn kernel by data\n",
    "\n",
    "最后我们来看一个例子，它使用物体边缘检测中的输入数据`X`和输出数据`Y`来学习我们构造的核数组`K`。我们首先构造一个卷积层，将其卷积核初始化成随机数组。接下来在每一次迭代中，我们使用平方误差来比较`Y`和卷积层的输出，然后计算梯度来更新权重。简单起见，这里的卷积层忽略了偏差。\n",
    "\n",
    "虽然我们之前构造了`Conv2D`类，但由于`corr2d`使用了对单个元素赋值（`[i, j]=`）的操作因而无法自动求梯度。下面我们使用tf.keras.layers提供的`Conv2D`类来实现这个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6, 7, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(X, (1,6,8,1))\n",
    "Y = tf.reshape(Y, (1,6,7,1))\n",
    "Y\n",
    "\n",
    "conv2d = tf.keras.layers.Conv2D(1, (1,2))\n",
    "#input_shape = (samples, rows, cols, channels)\n",
    "# Y = conv2d(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 2, 损失 29.929\n",
      "批次 4, 损失 11.018\n",
      "批次 6, 损失 4.305\n",
      "批次 8, 损失 1.728\n",
      "批次 10, 损失 0.702\n",
      "批次 12, 损失 0.287\n",
      "批次 14, 损失 0.117\n",
      "批次 16, 损失 0.048\n",
      "批次 18, 损失 0.020\n",
      "批次 20, 损失 0.008\n",
      "批次 22, 损失 0.003\n",
      "批次 24, 损失 0.001\n",
      "批次 26, 损失 0.001\n",
      "批次 28, 损失 0.000\n",
      "批次 30, 损失 0.000\n",
      "批次 32, 损失 0.000\n",
      "批次 34, 损失 0.000\n",
      "批次 36, 损失 0.000\n",
      "批次 38, 损失 0.000\n",
      "批次 40, 损失 0.000\n",
      "批次 42, 损失 0.000\n",
      "批次 44, 损失 0.000\n",
      "批次 46, 损失 0.000\n",
      "批次 48, 损失 0.000\n",
      "批次 50, 损失 0.000\n",
      "批次 52, 损失 0.000\n",
      "批次 54, 损失 0.000\n",
      "批次 56, 损失 0.000\n",
      "批次 58, 损失 0.000\n",
      "批次 60, 损失 0.000\n",
      "批次 62, 损失 0.000\n",
      "批次 64, 损失 0.000\n",
      "批次 66, 损失 0.000\n",
      "批次 68, 损失 0.000\n",
      "批次 70, 损失 0.000\n",
      "批次 72, 损失 0.000\n",
      "批次 74, 损失 0.000\n",
      "批次 76, 损失 0.000\n",
      "批次 78, 损失 0.000\n",
      "批次 80, 损失 0.000\n",
      "批次 82, 损失 0.000\n",
      "批次 84, 损失 0.000\n",
      "批次 86, 损失 0.000\n",
      "批次 88, 损失 0.000\n",
      "批次 90, 损失 0.000\n",
      "批次 92, 损失 0.000\n",
      "批次 94, 损失 0.000\n",
      "批次 96, 损失 0.000\n",
      "批次 98, 损失 0.000\n",
      "批次 100, 损失 0.000\n"
     ]
    }
   ],
   "source": [
    "# 初始化卷积层的输出\n",
    "Y_hat = conv2d(X)\n",
    "\n",
    "# 进行10次迭代训练\n",
    "for i in range(100):\n",
    "    # 使用GradientTape记录梯度\n",
    "    with tf.GradientTape() as g:\n",
    "        # 计算卷积层的输出\n",
    "        Y_hat = conv2d(X)\n",
    "        # 计算均方误差损失\n",
    "        l = tf.reduce_sum((Y_hat - Y) ** 2)\n",
    "        if l < 1e-3:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # 计算损失对卷积层参数的梯度\n",
    "    dl = g.gradient(l, conv2d.trainable_variables)\n",
    "    \n",
    "    # 设置学习率\n",
    "    lr = 3e-2\n",
    "    \n",
    "    # 反向传播，更新部分权重参数\n",
    "    conv2d.trainable_variables[0].assign_sub(lr * dl[0])\n",
    "    # 更新所有权重参数\n",
    "    # 实验表示更新全部权重会导致Loss发散\n",
    "    # 批次 2, 损失 58.790\n",
    "    # 批次 4, 损失 3209.656\n",
    "    # 批次 6, 损失 178068.516\n",
    "    # 批次 8, 损失 9879409.000\n",
    "    # 批次 10, 损失 548118784.000\n",
    "    # for j, grad in enumerate(dl):\n",
    "    #     conv2d.trainable_variables[j].assign_sub(lr * grad)\n",
    "    \n",
    "    # 每两次迭代打印一次损失\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print('批次 %d, 损失 %.3f' % (i + 1, l))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，10次迭代后误差已经降到了一个比较小的值。现在来看一下学习到的核数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 1.0000002, -0.9999999]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(conv2d.get_weights()[0],(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，学到的核数组与我们之前定义的核数组`K`较接近。\n",
    "\n",
    "## 互相关运算和卷积运算\n",
    "\n",
    "实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。\n",
    "\n",
    "那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出图5.1中的核数组。设其他条件不变，使用卷积运算学出的核数组即图5.1中的核数组按上下、左右翻转。也就是说，图5.1中的输入与学出的已翻转的核数组再做卷积运算时，依然得到图5.1中的输出。为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。\n",
    "\n",
    "\n",
    "## 特征图和感受野\n",
    "\n",
    "二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图5.1中形状为$2 \\times 2$的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。\n",
    "\n",
    "我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。当含义明确时，本书不对这两个术语做严格区分。\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。\n",
    "* 我们可以设计卷积核来检测图像中的边缘。\n",
    "* 我们可以通过数据来学习卷积核。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sonnet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
