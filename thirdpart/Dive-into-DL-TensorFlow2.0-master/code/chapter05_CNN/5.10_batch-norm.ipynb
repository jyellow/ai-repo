{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ac10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:01.958376Z",
     "iopub.status.busy": "2024-10-22T09:07:01.957989Z",
     "iopub.status.idle": "2024-10-22T09:07:14.872201Z",
     "shell.execute_reply": "2024-10-22T09:07:14.871224Z"
    },
    "papermill": {
     "duration": 12.923442,
     "end_time": "2024-10-22T09:07:14.874527",
     "exception": false,
     "start_time": "2024-10-22T09:07:01.951085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d8a0f",
   "metadata": {
    "papermill": {
     "duration": 0.004552,
     "end_time": "2024-10-22T09:07:14.884523",
     "exception": false,
     "start_time": "2024-10-22T09:07:14.879971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 批量归一化\n",
    "\n",
    "本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 [1]。在 “[实战Kaggle比赛：预测房价](http://zh.d2l.ai/chapter_deep-learning-basics/kaggle-house-price.html)” 一节里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。\n",
    "\n",
    "## 5.10.1 批量归一化层\n",
    "\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。\n",
    "\n",
    "### 5.10.1.1. 对全连接层做批量归一化\n",
    "我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$\\boldsymbol{u}$，权重参数和偏差参数分别为$\\boldsymbol{W}$和$\\boldsymbol{b}$，，激活函数为ϕ。设批量归一化的运算符为$\\boldsymbol{BN}$。那么，使用批量归一化的全连接层的输出为：\n",
    "$$\\phi(\\text{BN}(\\boldsymbol{x})),$$\n",
    "其中批量归一化输入由$\\boldsymbol{x}$仿射变换\n",
    "$$\\boldsymbol{x} = \\boldsymbol{W\\boldsymbol{u} + \\boldsymbol{b}}$$\n",
    "得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量$\\mathcal{B} = \\{\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(m)} \\}$。它们正是批量归一化层的输入。对于小批量$\\mathcal{B}$中任意样本$\\boldsymbol{x}^{(i)} \\in \\mathbb{R}^d, 1 \\leq i \\leq m$，批量归一化层的输出同样是$\\boldsymbol{d}$维向量\n",
    "$$\\boldsymbol{y}^{(i)} = \\text{BN}(\\boldsymbol{x}^{(i)}),$$\n",
    "并由以下几步求得。首先，对小批量$\\mathcal{B}$求均值和方差：\n",
    "$$\\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)},$$\n",
    "$$\\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,$$\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$\\boldsymbol{x}^{(i)}$标准化：\n",
    "$$\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}},$$\n",
    "这里$\\epsilon > 0$是一个很小的常数，保证分母大于0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数$\\boldsymbol{\\gamma}$和偏移（shift）参数$\\boldsymbol{\\beta}$。这两个参数和$\\boldsymbol{x}^{(i)}$形状相同，皆为d维向量。它们与$\\hat{\\boldsymbol{x}}^{(i)}$分别做按元素乘法（符号$\\odot$）和加法计算：\n",
    "$${\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.$$\n",
    "至此，我们得到了$\\boldsymbol{x}^{(i)}$的批量归一化的输出$\\boldsymbol{y}^{(i)}$。 值得注意的是，可学习的拉伸和偏移参数保留了不对$\\boldsymbol{x}^{(i)}$做批量归一化的可能：此时只需学出$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}_\\mathcal{B}$。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。\n",
    "\n",
    "### 5.10.1.2. 对卷积层做批量归一化\n",
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有 m个样本。在单个通道上，假设卷积计算输出的高和宽分别为 p和 q 。我们需要对该通道中 m×p×q个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 m×p×q个元素的均值和方差。\n",
    "\n",
    "### 5.10.1.3. 预测时的批量归一化\n",
    "使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。\n",
    "\n",
    "## 5.10.2. 从零开始实现\n",
    "下面我们通过numpy中的ndarray来实现批量归一化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22d177",
   "metadata": {},
   "source": [
    "这段代码实现了批量归一化(Batch Normalization)操作，是深度学习中一个重要的网络层。让我逐步解释它的功能：\n",
    "\n",
    "1. 函数接收以下参数：\n",
    "   - is_training：标识是训练模式还是预测模式\n",
    "   - X：输入数据\n",
    "   - gamma：缩放参数\n",
    "   - beta：偏移参数\n",
    "   - moving_mean：移动平均的均值\n",
    "   - moving_var：移动平均的方差\n",
    "   - eps：一个很小的数，防止除零\n",
    "   - momentum：动量参数，用于移动平均的更新\n",
    "\n",
    "2. 函数的主要逻辑分两种模式：\n",
    "\n",
    "   预测模式 (is_training = False):\n",
    "   - 直接使用已经计算好的移动平均均值和方差来归一化数据\n",
    "\n",
    "   训练模式 (is_training = True):\n",
    "   - 计算当前批次的均值和方差\n",
    "   - 支持两种数据形状：\n",
    "     * 二维数据(全连接层的情况)：在特征维度上计算统计量\n",
    "     * 四维数据(卷积层的情况)：在通道维度上计算统计量\n",
    "   - 使用当前批次的统计量进行归一化\n",
    "   - 更新移动平均的均值和方差\n",
    "\n",
    "3. 最后的步骤：\n",
    "   - 对归一化后的数据进行缩放和偏移（使用gamma和beta参数）\n",
    "   - 返回处理后的数据和更新后的移动平均值\n",
    "\n",
    "批量归一化的主要目的是：\n",
    "- 减少内部协变量偏移\n",
    "- 加速网络训练\n",
    "- 允许使用更大的学习率\n",
    "- 减少对初始化的依赖\n",
    "- 具有轻微正则化效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bd0607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:14.895490Z",
     "iopub.status.busy": "2024-10-22T09:07:14.894943Z",
     "iopub.status.idle": "2024-10-22T09:07:14.904479Z",
     "shell.execute_reply": "2024-10-22T09:07:14.903641Z"
    },
    "papermill": {
     "duration": 0.016967,
     "end_time": "2024-10-22T09:07:14.906366",
     "exception": false,
     "start_time": "2024-10-22T09:07:14.889399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_norm(is_training,X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断是当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(axis=0)\n",
    "            var = ((X - mean) ** 2).mean(axis=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / np.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737b21a",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2024-10-22T09:07:14.915783",
     "exception": false,
     "start_time": "2024-10-22T09:07:14.911110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "接下来，我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b7ba9",
   "metadata": {},
   "source": [
    "自定义的 BatchNormalization 层\n",
    "\n",
    "1. 类的初始化（__init__）:\n",
    "   - 接收两个主要参数：decay（衰减率，默认0.9）和 epsilon（防止除零的小数值，默认1e-5）\n",
    "   - 继承自 tf.keras.layers.Layer 基类\n",
    "\n",
    "2. 构建层（build 方法）:\n",
    "   创建四个关键参数：\n",
    "   - gamma：可训练的缩放参数，初始化为1\n",
    "   - beta：可训练的偏移参数，初始化为0\n",
    "   - moving_mean：不可训练的移动平均均值，初始化为0\n",
    "   - moving_variance：不可训练的移动平均方差，初始化为1\n",
    "\n",
    "3. 移动平均计算（assign_moving_average 方法）:\n",
    "   - 实现指数移动平均的更新\n",
    "   - 公式：new_value = old_value * decay + current_value * (1 - decay)\n",
    "\n",
    "4. 核心计算逻辑（call 方法）:\n",
    "   分为训练和推理两种模式：\n",
    "   \n",
    "   训练模式：\n",
    "   - 计算当前批次的均值和方差\n",
    "   - 更新移动平均值\n",
    "   - 使用当前批次的统计量进行归一化\n",
    "\n",
    "   推理模式：\n",
    "   - 直接使用已经计算好的移动平均值\n",
    "   \n",
    "   最后：\n",
    "   - 使用 tf.nn.batch_normalization 进行标准化操作\n",
    "   - 应用 gamma（缩放）和 beta（偏移）参数\n",
    "\n",
    "5. 输出形状计算（compute_output_shape 方法）:\n",
    "   - 保持输入形状不变\n",
    "\n",
    "这个实现展示了批量归一化的核心原理：\n",
    "- 在训练时对每个批次进行归一化\n",
    "- 维护移动平均统计量用于推理\n",
    "- 包含可学习的缩放和偏移参数\n",
    "- 区分训练和推理两种模式的不同行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4fbbc",
   "metadata": {},
   "source": [
    "gamma、beta、moving_mean和moving_variance这四个参数在batch normalization中的作用\n",
    "\n",
    "1. **gamma (缩放参数)**\n",
    "   - 是一个可训练参数\n",
    "   - 用途：控制归一化后数据的缩放程度\n",
    "   - 作用：\n",
    "     * 让网络可以学习恢复数据的原始分布（如果需要的话）\n",
    "     * 如果gamma=1，保持标准化的尺度\n",
    "     * 如果gamma>1，扩大数据分布范围\n",
    "     * 如果gamma<1，压缩数据分布范围\n",
    "   - 初始化通常设为1\n",
    "\n",
    "2. **beta (偏移参数)**\n",
    "   - 是一个可训练参数\n",
    "   - 用途：控制归一化后数据的偏移量\n",
    "   - 作用：\n",
    "     * 让网络可以学习数据的最优偏移量\n",
    "     * 可以整体平移数据分布\n",
    "     * 补偿归一化可能带来的数据分布改变\n",
    "   - 初始化通常设为0\n",
    "\n",
    "3. **moving_mean (移动平均均值)**\n",
    "   - 是一个不可训练参数\n",
    "   - 用途：记录训练过程中所有批次的均值的指数移动平均\n",
    "   - 作用：\n",
    "     * 在推理阶段使用，提供稳定的归一化参数\n",
    "     * 消除批次间的波动影响\n",
    "     * 使预测结果更加稳定\n",
    "   - 初始化为0，在训练过程中不断更新\n",
    "\n",
    "4. **moving_variance (移动平均方差)**\n",
    "   - 是一个不可训练参数\n",
    "   - 用途：记录训练过程中所有批次的方差的指数移动平均\n",
    "   - 作用：\n",
    "     * 在推理阶段使用，提供稳定的归一化参数\n",
    "     * 消除批次间的波动影响\n",
    "     * 使预测结果更加稳定\n",
    "   - 初始化为1，在训练过程中不断更新\n",
    "\n",
    "这些参数的协同作用：\n",
    "1. **训练阶段**：\n",
    "   - 使用当前批次的均值和方差进行归一化\n",
    "   - 同时更新moving_mean和moving_variance\n",
    "   - 应用gamma和beta进行缩放和偏移\n",
    "\n",
    "2. **推理阶段**：\n",
    "   - 使用moving_mean和moving_variance进行归一化\n",
    "   - 应用训练好的gamma和beta进行缩放和偏移\n",
    "\n",
    "这种设计的优点：\n",
    "- 保持了网络的表达能力\n",
    "- 提供了稳定的推理结果\n",
    "- 允许模型学习最优的数据分布\n",
    "- 解决了内部协变量偏移问题\n",
    "\n",
    "总的来说，这四个参数共同确保了batch normalization既能在训练时提供良好的正则化效果，又能在推理时保持稳定的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cb5b8",
   "metadata": {},
   "source": [
    "### 让我用简单的方式解释\"偏移量\"：\n",
    "\n",
    "想象一下在坐标系中的一条曲线：\n",
    "1. 当你给这条曲线加上一个正的beta值，整条曲线会向上移动\n",
    "2. 当你给这条曲线加上一个负的beta值，整条曲线会向下移动\n",
    "\n",
    "举个具体的例子：\n",
    "- 假设经过标准化后，某个神经元的输出值是1.0\n",
    "- 如果beta = 0.5，那么最终输出就变成了1.5\n",
    "- 如果beta = -0.5，那么最终输出就变成了0.5\n",
    "\n",
    "简单来说，beta就像是一个\"调整器\"：\n",
    "- 它可以整体提升或降低数据的数值\n",
    "- 这种调整是统一的，对所有数据都加上相同的值\n",
    "- 这样的调整有助于网络找到数据的最佳位置\n",
    "\n",
    "用生活中的例子：\n",
    "- 如果把数据比作电视的画面\n",
    "- beta就像是画面的亮度调节\n",
    "- 可以让整个画面统一变亮或变暗\n",
    "\n",
    "所以，\"偏移量\"就是整体平移数据的一个值，让数据整体上移或下移。\n",
    "\n",
    "### 用非技术的语言介绍batch normalization\n",
    "\n",
    "让我用生活化的例子来解释Batch Normalization：\n",
    "\n",
    "想象你是一个老师，负责评分：\n",
    "\n",
    "1. **标准化过程**\n",
    "- 一个班级100人参加考试，分数分布很不均匀，有的98分，有的30分\n",
    "- 为了更好地评估学生水平，你决定\"标准化\"这些分数：\n",
    "  * 先算出班级平均分（比如70分）\n",
    "  * 看看每个人比平均分高或低多少\n",
    "  * 最后把分数调整到一个合理的范围内\n",
    "\n",
    "2. **为什么要这样做？**\n",
    "- 让评分更公平：不同难度的考试可以比较\n",
    "- 更容易发现问题：清楚地看出谁在平均水平之上或之下\n",
    "- 便于后续处理：调整后的分数更容易进行进一步分析\n",
    "\n",
    "3. **moving_mean和moving_variance的作用**\n",
    "就像老师会记录：\n",
    "- 这个班级历次考试的平均水平\n",
    "- 不会只看一次考试的情况\n",
    "- 通过多次考试的记录，更准确地了解班级整体水平\n",
    "\n",
    "4. **gamma和beta的作用**\n",
    "相当于老师可以：\n",
    "- 根据需要调整分数的分布范围（gamma）\n",
    "- 适当提高或降低整体分数（beta）\n",
    "- 使最终的分数更合理，更有意义\n",
    "\n",
    "5. **训练和预测的区别**\n",
    "- 训练时：就像平时考试，每次都要算当次的平均分和分布\n",
    "- 预测时：就像期末总评，要参考学生在整个学期的表现\n",
    "\n",
    "这种方法的好处：\n",
    "- 让评分更稳定\n",
    "- 更容易发现学生的真实水平\n",
    "- 便于跟其他班级比较\n",
    "- 有助于后续的教学调整\n",
    "\n",
    "这就是Batch Normalization的基本思想：通过标准化和适当的调整，让数据更容易被处理，最终得到更好的结果。\n",
    "\n",
    "### 用非技术语言描述gamma参数如何使用和训练\n",
    "\n",
    "让我用一个体育课测试的例子来解释gamma参数：\n",
    "\n",
    "想象你是体育老师，在测量学生的跳远成绩：\n",
    "\n",
    "1. **基本情况**\n",
    "- 已经把所有学生的成绩标准化了（比如都调整到0-1之间）\n",
    "- gamma就像是一个\"放大镜\"或\"缩小镜\"\n",
    "\n",
    "2. **gamma的作用**\n",
    "- gamma = 1: 保持现状，成绩就维持标准化后的样子\n",
    "- gamma > 1: 放大差异，比如：\n",
    "  * 原本相差0.1分的两个成绩\n",
    "  * 可能会被放大到相差0.3分\n",
    "- gamma < 1: 缩小差异，比如：\n",
    "  * 原本相差0.1分的两个成绩\n",
    "  * 可能会被缩小到相差0.03分\n",
    "\n",
    "3. **gamma如何训练**\n",
    "就像老师在不断调整评分标准：\n",
    "- 一开始设定gamma = 1\n",
    "- 通过观察学生的表现，逐渐调整：\n",
    "  * 如果发现成绩差异太小，不容易区分，就增大gamma\n",
    "  * 如果发现成绩差异太大，不够合理，就减小gamma\n",
    "- 这个调整是自动的，系统会根据需要找到最合适的值\n",
    "\n",
    "4. **为什么需要gamma**\n",
    "就像在评分时：\n",
    "- 有时需要放大差异，让优秀和不足更明显\n",
    "- 有时需要缩小差异，让评分更温和\n",
    "- gamma提供了这种灵活性\n",
    "\n",
    "简单说：\n",
    "- gamma就是一个可以自动调节的\"放大镜\"\n",
    "- 它帮助系统找到最合适的区分度\n",
    "- 这个过程是自动的，不需要人工干预\n",
    "\n",
    "### 使用非技术语言描述beta参数如何作用以及训练\n",
    "\n",
    "让我继续用体育课测试的例子来解释beta参数：\n",
    "\n",
    "1. **基本情况**\n",
    "想象你是体育老师，在给跳远成绩评分：\n",
    "- 所有成绩已经标准化了（假设都在0分附近）\n",
    "- beta就像是一个\"整体加分\"或\"整体减分\"的工具\n",
    "\n",
    "2. **beta的作用**\n",
    "就像调整基础分：\n",
    "- beta = 0：保持原样，不加不减\n",
    "- beta = 2：所有人都加2分\n",
    "- beta = -1：所有人都减1分\n",
    "\n",
    "举个具体例子：\n",
    "- 小明原始分数是0.5\n",
    "- 如果beta = 2，小明最终得分就是2.5\n",
    "- 如果beta = -1，小明最终得分就是-0.5\n",
    "- 这种加减应用于所有学生\n",
    "\n",
    "3. **beta如何训练**\n",
    "就像老师在不断调整基础分：\n",
    "- 一开始设定beta = 0（不加不减）\n",
    "- 系统会自动观察并调整：\n",
    "  * 如果发现分数整体偏低，就增加beta\n",
    "  * 如果发现分数整体偏高，就减少beta\n",
    "- 这个过程是自动的，系统会找到最合适的加减分值\n",
    "\n",
    "4. **为什么需要beta**\n",
    "就像在实际评分中：\n",
    "- 有时题目太难，需要适当加分\n",
    "- 有时题目太简单，需要适当减分\n",
    "- beta提供了整体调节的能力\n",
    "\n",
    "简单说：\n",
    "- beta就像一个\"统一加减分\"的工具\n",
    "- 它能整体提升或降低所有分数\n",
    "- 这个加减分的数值是自动学习得到的\n",
    "- 目的是让最终分数更合理\n",
    "\n",
    "生活中的类比：\n",
    "- 就像调整电视机的亮度\n",
    "- beta就是让画面整体变亮或变暗的旋钮\n",
    "- 系统会自动找到最舒服的亮度水平\n",
    "\n",
    "\n",
    "总结一下我们讨论的要点：\n",
    "1. Batch Normalization 就像是一个智能的评分系统\n",
    "2. gamma 像是一个自动调节的\"放大镜\"，控制差异的程度\n",
    "3. beta 像是一个\"整体加减分\"的工具，进行统一调整\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464c2969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:14.927851Z",
     "iopub.status.busy": "2024-10-22T09:07:14.927560Z",
     "iopub.status.idle": "2024-10-22T09:07:14.994140Z",
     "shell.execute_reply": "2024-10-22T09:07:14.993435Z"
    },
    "papermill": {
     "duration": 0.0746,
     "end_time": "2024-10-22T09:07:14.996095",
     "exception": false,
     "start_time": "2024-10-22T09:07:14.921495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 自定义BatchNormalization层，代码运行不起来，暂时跳过\n",
    "# 仅用来理解原理\n",
    "class BatchNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, decay=0.9, epsilon=1e-5, **kwargs):\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma',\n",
    "                                     shape=[input_shape[-1], ],\n",
    "                                     initializer=tf.initializers.ones,\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                    shape=[input_shape[-1], ],\n",
    "                                    initializer=tf.initializers.zeros,\n",
    "                                    trainable=True)\n",
    "        self.moving_mean = self.add_weight(name='moving_mean',\n",
    "                                           shape=[input_shape[-1], ],\n",
    "                                           initializer=tf.initializers.zeros,\n",
    "                                           trainable=False)\n",
    "        self.moving_variance = self.add_weight(name='moving_variance',\n",
    "                                               shape=[input_shape[-1], ],\n",
    "                                               initializer=tf.initializers.ones,\n",
    "                                               trainable=False)\n",
    "        super(BatchNormalization, self).build(input_shape)\n",
    "\n",
    "    def assign_moving_average(self, variable, value):\n",
    "        \"\"\"\n",
    "        variable = variable * decay + value * (1 - decay)\n",
    "        \"\"\"\n",
    "        delta = variable * self.decay + value * (1 - self.decay)\n",
    "        return variable.assign(delta)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training):\n",
    "        if training:\n",
    "            batch_mean, batch_variance = tf.nn.moments(inputs, list(range(len(inputs.shape) - 1)))\n",
    "            mean_update = self.assign_moving_average(self.moving_mean, batch_mean)\n",
    "            variance_update = self.assign_moving_average(self.moving_variance, batch_variance)\n",
    "            self.add_update(mean_update)\n",
    "            self.add_update(variance_update)\n",
    "            mean, variance = batch_mean, batch_variance\n",
    "        else:\n",
    "            mean, variance = self.moving_mean, self.moving_variance\n",
    "        output = tf.nn.batch_normalization(inputs,\n",
    "                                           mean=mean,\n",
    "                                           variance=variance,\n",
    "                                           offset=self.beta,\n",
    "                                           scale=self.gamma,\n",
    "                                           variance_epsilon=self.epsilon)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ff701",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2024-10-22T09:07:15.005413",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.000857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.10.2.1. 使用批量归一化层的LeNet\n",
    "\n",
    "下面我们修改“[卷积神经网络（LeNet）](http://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)”这一节介绍的LeNet模型，从而应用批量归一化层。我们在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6999a2",
   "metadata": {},
   "source": [
    "### 添加Batch Normalization后的LeNet网络模型\n",
    "\n",
    "1. **输入层**\n",
    "- 接收28×28大小的灰度图片（1个通道）\n",
    "\n",
    "2. **第一个卷积块**\n",
    "- 使用6个5×5的卷积核扫描图片\n",
    "- 用Batch Normalization进行数据标准化\n",
    "- 使用sigmoid激活函数增加非线性\n",
    "- 用2×2的最大池化层缩小特征图尺寸\n",
    "\n",
    "3. **第二个卷积块**\n",
    "- 使用16个5×5的卷积核进行特征提取\n",
    "- 同样进行Batch Normalization标准化\n",
    "- 使用sigmoid激活函数\n",
    "- 再次用2×2的最大池化层减小尺寸\n",
    "\n",
    "4. **扁平化操作**\n",
    "- 将二维特征图展平成一维向量\n",
    "\n",
    "5. **第一个全连接层**\n",
    "- 连接到120个神经元\n",
    "- 使用Batch Normalization\n",
    "- 使用sigmoid激活函数\n",
    "\n",
    "6. **第二个全连接层**\n",
    "- 连接到84个神经元\n",
    "- 使用Batch Normalization\n",
    "- 使用sigmoid激活函数\n",
    "\n",
    "7. **输出层**\n",
    "- 最后输出10个数值（可能用于10分类问题）\n",
    "- 使用sigmoid激活函数\n",
    "\n",
    "主要特点：\n",
    "- 每个主要层后都加入了Batch Normalization，有助于训练\n",
    "- 使用了典型的\"卷积+池化\"结构提取特征\n",
    "- 全部使用sigmoid激活函数\n",
    "- 整体结构是\"窄而深\"的，逐层提取更抽象的特征\n",
    "\n",
    "这个网络结构适合处理简单的图像分类任务，比如MNIST手写数字识别。\n",
    "\n",
    "### 计算该神经网络模型每一层的训练参数\n",
    "\n",
    "1. **第一个卷积层**\n",
    "- 卷积核：5×5×1×6\n",
    "- 参数量 = (5×5×1)×6 + 6(偏置) = 156个参数\n",
    "\n",
    "2. **第一个Batch Normalization层**\n",
    "- 每个通道有gamma和beta两个参数\n",
    "- 参数量 = 6×2 = 12个参数\n",
    "\n",
    "3. **第二个卷积层**\n",
    "- 卷积核：5×5×6×16\n",
    "- 参数量 = (5×5×6)×16 + 16(偏置) = 2,416个参数\n",
    "\n",
    "4. **第二个Batch Normalization层**\n",
    "- 每个通道有gamma和beta两个参数\n",
    "- 参数量 = 16×2 = 32个参数\n",
    "\n",
    "5. **第一个全连接层（Dense-120）**\n",
    "- 输入是扁平化后的特征图\n",
    "- 假设输入大小为N（需要计算）\n",
    "- 参数量 = N×120 + 120(偏置)\n",
    "- [需要计算输入尺寸N]\n",
    "\n",
    "6. **第三个Batch Normalization层**\n",
    "- 参数量 = 120×2 = 240个参数\n",
    "\n",
    "7. **第二个全连接层（Dense-84）**\n",
    "- 参数量 = 120×84 + 84(偏置) = 10,164个参数\n",
    "\n",
    "8. **第四个Batch Normalization层**\n",
    "- 参数量 = 84×2 = 168个参数\n",
    "\n",
    "9. **输出层（Dense-10）**\n",
    "- 参数量 = 84×10 + 10(偏置) = 850个参数\n",
    "\n",
    "为了计算完整的参数量，我们需要计算第一个全连接层的输入尺寸：\n",
    "1. 输入图片：28×28\n",
    "2. 第一次卷积(5×5)：24×24×6\n",
    "3. 第一次池化(2×2)：12×12×6\n",
    "4. 第二次卷积(5×5)：8×8×16\n",
    "5. 第二次池化(2×2)：4×4×16\n",
    "6. 扁平化后：4×4×16 = 256\n",
    "\n",
    "所以第一个全连接层的参数量：\n",
    "- 参数量 = 256×120 + 120 = 30,840个参数\n",
    "\n",
    "总参数量：\n",
    "156 + 12 + 2,416 + 32 + 30,840 + 240 + 10,164 + 168 + 850 = 44,878个参数\n",
    "\n",
    "请注意：\n",
    "- BN层还有两个不可训练参数（moving_mean和moving_variance）\n",
    "- 实际使用时可以用model.summary()查看准确的参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2b1e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:15.016011Z",
     "iopub.status.busy": "2024-10-22T09:07:15.015703Z",
     "iopub.status.idle": "2024-10-22T09:07:15.019988Z",
     "shell.execute_reply": "2024-10-22T09:07:15.019136Z"
    },
    "papermill": {
     "duration": 0.011816,
     "end_time": "2024-10-22T09:07:15.021988",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.010172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(filters=6, kernel_size=5)(inputs)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=16, kernel_size=5)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(120)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "x = tf.keras.layers.Dense(84)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='sigmoid')(x)\n",
    "\n",
    "net = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362d1da",
   "metadata": {
    "papermill": {
     "duration": 0.004604,
     "end_time": "2024-10-22T09:07:15.031530",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.026926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "下面我们训练修改后的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a6807f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:15.042605Z",
     "iopub.status.busy": "2024-10-22T09:07:15.042343Z",
     "iopub.status.idle": "2024-10-22T09:07:15.046392Z",
     "shell.execute_reply": "2024-10-22T09:07:15.045531Z"
    },
    "papermill": {
     "duration": 0.011824,
     "end_time": "2024-10-22T09:07:15.048292",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.036468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "# x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# net.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer=tf.keras.optimizers.RMSprop(),\n",
    "#               metrics=['accuracy'])\n",
    "# history = net.fit(x_train, y_train,\n",
    "#                     batch_size=64,\n",
    "#                     epochs=5,\n",
    "#                     validation_split=0.2)\n",
    "# test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "# print('Test loss:', test_scores[0])\n",
    "# print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980da6ad",
   "metadata": {
    "papermill": {
     "duration": 0.004791,
     "end_time": "2024-10-22T09:07:15.057862",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.053071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "最后我们查看第一个批量归一化层学习到的拉伸参数gamma和偏移参数beta。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf397d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:15.069090Z",
     "iopub.status.busy": "2024-10-22T09:07:15.068605Z",
     "iopub.status.idle": "2024-10-22T09:07:15.072391Z",
     "shell.execute_reply": "2024-10-22T09:07:15.071552Z"
    },
    "papermill": {
     "duration": 0.011365,
     "end_time": "2024-10-22T09:07:15.074258",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.062893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net.get_layer(index=1).gamma,net.get_layer(index=1).beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddb32e",
   "metadata": {
    "papermill": {
     "duration": 0.004647,
     "end_time": "2024-10-22T09:07:15.083942",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.079295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.10.3 简洁实现\n",
    "\n",
    "与我们刚刚自己定义的`BatchNorm`类相比，keras中`layers`模块定义的`BatchNorm`类使用起来更加简单。它不需要指定自己定义的`BatchNorm`类中所需的`num_features`和`num_dims`参数值。在keras中，这些参数值都将通过延后初始化而自动获取。下面我们用keras实现使用批量归一化的LeNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ddeb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:15.095401Z",
     "iopub.status.busy": "2024-10-22T09:07:15.094603Z",
     "iopub.status.idle": "2024-10-22T09:07:15.120705Z",
     "shell.execute_reply": "2024-10-22T09:07:15.119833Z"
    },
    "papermill": {
     "duration": 0.03358,
     "end_time": "2024-10-22T09:07:15.122620",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.089040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential()\n",
    "net.add(tf.keras.layers.Conv2D(filters=6,kernel_size=5))\n",
    "net.add(tf.keras.layers.BatchNormalization())\n",
    "net.add(tf.keras.layers.Activation('sigmoid'))\n",
    "net.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "net.add(tf.keras.layers.Conv2D(filters=16,kernel_size=5))\n",
    "net.add(tf.keras.layers.BatchNormalization())\n",
    "net.add(tf.keras.layers.Activation('sigmoid'))\n",
    "net.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "net.add(tf.keras.layers.Flatten())\n",
    "net.add(tf.keras.layers.Dense(120))\n",
    "net.add(tf.keras.layers.BatchNormalization())\n",
    "net.add(tf.keras.layers.Activation('sigmoid'))\n",
    "net.add(tf.keras.layers.Dense(84))\n",
    "net.add(tf.keras.layers.BatchNormalization())\n",
    "net.add(tf.keras.layers.Activation('sigmoid'))\n",
    "net.add(tf.keras.layers.Dense(10,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901e1c5",
   "metadata": {
    "papermill": {
     "duration": 0.004669,
     "end_time": "2024-10-22T09:07:15.132314",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.127645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "使用同样的超参数进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b13525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T09:07:15.143339Z",
     "iopub.status.busy": "2024-10-22T09:07:15.143048Z",
     "iopub.status.idle": "2024-10-22T09:07:32.436478Z",
     "shell.execute_reply": "2024-10-22T09:07:32.435463Z"
    },
    "papermill": {
     "duration": 17.301309,
     "end_time": "2024-10-22T09:07:32.438621",
     "exception": false,
     "start_time": "2024-10-22T09:07:15.137312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "net.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(),\n",
    "            metrics=['accuracy'])\n",
    "history = net.fit(x_train, y_train,\n",
    "                  batch_size=64,\n",
    "                  epochs=5,\n",
    "                  validation_split=0.2)\n",
    "test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86f0f9",
   "metadata": {
    "papermill": {
     "duration": 0.017779,
     "end_time": "2024-10-22T09:07:32.475330",
     "exception": false,
     "start_time": "2024-10-22T09:07:32.457551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.10.4 小结\n",
    "\n",
    "* 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n",
    "* 对全连接层和卷积层做批量归一化的方法稍有不同。\n",
    "* 批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。\n",
    "* keras提供的BatchNorm类使用起来简单、方便。\n",
    "\n",
    "\n",
    "## 5.10.4 练习\n",
    "\n",
    "* 能否将批量归一化前的全连接层或卷积层中的偏差参数去掉？为什么？（提示：回忆批量归一化中标准化的定义。）\n",
    "* 尝试调大学习率。同[“卷积神经网络（LeNet）”](lenet.ipynb)一节中未使用批量归一化的LeNet相比，现在是不是可以使用更大的学习率？\n",
    "* 尝试将批量归一化层插入LeNet的其他地方，观察并分析结果的变化。\n",
    "* 尝试一下不学习拉伸参数`gamma`和偏移参数`beta`（构造的时候加入参数`grad_req='null'`来避免计算梯度），观察并分析结果。\n",
    "* 查看`BatchNorm`类的文档来了解更多使用方法，例如，如何在训练时使用基于全局平均的均值和方差。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 5.10.4 参考文献\n",
    "\n",
    "[1] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe35df",
   "metadata": {
    "papermill": {
     "duration": 0.017645,
     "end_time": "2024-10-22T09:07:32.510899",
     "exception": false,
     "start_time": "2024-10-22T09:07:32.493254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.707563,
   "end_time": "2024-10-22T09:07:34.941228",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-22T09:06:59.233665",
   "version": "2.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
