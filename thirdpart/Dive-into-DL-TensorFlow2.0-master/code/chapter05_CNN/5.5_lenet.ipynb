{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:53:55.452955Z","iopub.status.busy":"2024-10-21T13:53:55.452629Z","iopub.status.idle":"2024-10-21T13:54:07.633637Z","shell.execute_reply":"2024-10-21T13:54:07.632658Z","shell.execute_reply.started":"2024-10-21T13:53:55.452920Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.16.1\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:17.291548Z","iopub.status.busy":"2024-10-21T13:54:17.291129Z","iopub.status.idle":"2024-10-21T13:54:17.297907Z","shell.execute_reply":"2024-10-21T13:54:17.296933Z","shell.execute_reply.started":"2024-10-21T13:54:17.291504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["当前可用的GPU设备: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n","当前可用的GPU设备: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"]}],"source":["gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    for gpu in gpus:\n","        tf.config.experimental.set_memory_growth(gpu, True)\n","        print(\"当前可用的GPU设备:\", gpu)  # 输出当前的GPU设备\n","        # tf.config.set_visible_devices(gpu, 'GPU')\n","else:\n","    print(\"没有检测到可用的GPU设备。\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# 卷积神经网络（LeNet）\n","\n","在[“多层感知机的从零开始实现”](../chapter_deep-learning-basics/mlp-scratch.ipynb)一节里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。\n","\n","1. 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n","2. 对于大尺寸的输入图像，使用全连接层容易导致模型过大。假设输入是高和宽均为$1,000$像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状也是$3,000,000\\times 256$：它占用了大约3 GB的内存或显存。这会带来过于复杂的模型和过高的存储开销。\n","\n","卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n","\n","卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet [1]。这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。\n","\n","## 5.5.1 LeNet模型\n","\n","LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n","\n","卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。\n","\n","卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n","\n","下面我们通过`Sequential`类来实现LeNet模型。"]},{"cell_type":"markdown","metadata":{},"source":["1. **导入必要组件**：\n","```python\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Input\n","```\n","2. **网络结构定义**：\n","```python\n","net = Sequential([\n","    Input(shape=(28, 28, 1)),  # 输入层：28x28的灰度图像\n","```\n","3. **卷积块1**：\n","```python\n","    Conv2D(filters=6, kernel_size=5, activation='sigmoid'),  # 第一个卷积层\n","    MaxPool2D(pool_size=2, strides=2),  # 第一个池化层\n","```\n","特点：\n","- 6个5×5的卷积核\n","- 2×2最大池化，步幅为2\n","- sigmoid激活函数\n","\n","4. **卷积块2**：\n","```python\n","    Conv2D(filters=16, kernel_size=5, activation='sigmoid'),  # 第二个卷积层\n","    MaxPool2D(pool_size=2, strides=2),  # 第二个池化层\n","```\n","特点：\n","- 16个5×5的卷积核\n","- 2×2最大池化，步幅为2\n","- sigmoid激活函数\n","\n","5. **全连接层**：\n","```python\n","    Flatten(),  # 展平层\n","    Dense(120, activation='sigmoid'),  # 第一个全连接层\n","    Dense(84, activation='sigmoid'),   # 第二个全连接层\n","    Dense(10, activation='sigmoid')    # 输出层\n","```\n","特点：\n","- 展平卷积特征\n","- 三层全连接网络\n","- 最终输出10个类别\n","\n","网络特点：\n","1. **层次结构**：\n","   - 2个卷积-池化块\n","   - 3个全连接层\n","   - 经典的LeNet-5架构\n","\n","2. **激活函数**：\n","   - 全部使用sigmoid激活\n","   - 符合历史版本设计\n","\n","3. **应用场景**：\n","   - 手写数字识别\n","   - 简单图像分类\n","   - 基础CNN教学\n","\n","这个网络是深度学习历史上的重要里程碑：\n","- 首个成功的CNN架构之一\n","- 展示了CNN的基本组件\n","- 为现代CNN奠定了基础\n","\n","注意：现代实现可能会：\n","- 使用ReLU激活函数\n","- 添加批量归一化\n","- 使用更现代的优化器"]},{"cell_type":"markdown","metadata":{},"source":["LeNet-5每一层的参数数量：\n","\n","1. **第一个卷积层**：\n","```python\n","Conv2D(filters=6, kernel_size=5, activation='sigmoid')\n","```\n","参数计算：\n","- 卷积核参数：5 × 5 × 1 × 6 = 150\n","  - 5×5是卷积核大小\n","  - 1是输入通道数\n","  - 6是输出通道数（filters）\n","- 偏置参数：6（每个卷积核一个）\n","- 总参数：156\n","\n","2. **第一个池化层**：\n","```python\n","MaxPool2D(pool_size=2, strides=2)\n","```\n","- 参数数量：0（池化层没有可训练参数）\n","\n","3. **第二个卷积层**：\n","```python\n","Conv2D(filters=16, kernel_size=5, activation='sigmoid')\n","```\n","参数计算：\n","- 卷积核参数：5 × 5 × 6 × 16 = 2400\n","  - 5×5是卷积核大小\n","  - 6是输入通道数（上一层的filters）\n","  - 16是输出通道数\n","- 偏置参数：16\n","- 总参数：2416\n","\n","4. **第二个池化层**：\n","```python\n","MaxPool2D(pool_size=2, strides=2)\n","```\n","- 参数数量：0\n","\n","5. **展平层**：\n","```python\n","Flatten()\n","```\n","- 参数数量：0（只改变形状，不涉及计算）\n","\n","6. **第一个全连接层**：\n","```python\n","Dense(120, activation='sigmoid')\n","```\n","参数计算：\n","- 输入维度：16 × 4 × 4 = 256（上一层展平后的大小）\n","- 权重参数：256 × 120 = 30720\n","- 偏置参数：120\n","- 总参数：30840\n","\n","7. **第二个全连接层**：\n","```python\n","Dense(84, activation='sigmoid')\n","```\n","参数计算：\n","- 权重参数：120 × 84 = 10080\n","- 偏置参数：84\n","- 总参数：10164\n","\n","8. **输出层**：\n","```python\n","Dense(10, activation='sigmoid')\n","```\n","参数计算：\n","- 权重参数：84 × 10 = 840\n","- 偏置参数：10\n","- 总参数：850\n","\n","总参数统计：\n","- 第一卷积层：156\n","- 第二卷积层：2,416\n","- 第一全连接层：30,840\n","- 第二全连接层：10,164\n","- 输出层：850\n","- 总计：44,426个参数\n","\n","观察：\n","1. 参数主要集中在全连接层\n","2. 卷积层参数相对较少\n","3. 池化层和展平层无参数\n","\n","这种参数分布是典型的CNN特征：\n","- 卷积层通过参数共享减少参数量\n","- 全连接层往往占据大部分参数\n","- 总参数量相对现代网络较少"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:31.376677Z","iopub.status.busy":"2024-10-21T13:54:31.376292Z","iopub.status.idle":"2024-10-21T13:54:31.813012Z","shell.execute_reply":"2024-10-21T13:54:31.812146Z","shell.execute_reply.started":"2024-10-21T13:54:31.376642Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Input\n","net = Sequential([\n","    Input(shape=(28, 28, 1)),\n","    Conv2D(filters=6, kernel_size=5, activation='sigmoid'),\n","    MaxPool2D(pool_size=2, strides=2),\n","    Conv2D(filters=16, kernel_size=5, activation='sigmoid'),\n","    MaxPool2D(pool_size=2, strides=2),\n","    Flatten(),\n","    Dense(120, activation='sigmoid'),\n","    Dense(84, activation='sigmoid'),\n","    Dense(10, activation='sigmoid')\n","])"]},{"cell_type":"markdown","metadata":{},"source":["接下来我们构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:35.574912Z","iopub.status.busy":"2024-10-21T13:54:35.574522Z","iopub.status.idle":"2024-10-21T13:54:36.796425Z","shell.execute_reply":"2024-10-21T13:54:36.795411Z","shell.execute_reply.started":"2024-10-21T13:54:35.574876Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["conv2d output shape\t (1, 24, 24, 6)\n","max_pooling2d output shape\t (1, 12, 12, 6)\n","conv2d_1 output shape\t (1, 8, 8, 16)\n","max_pooling2d_1 output shape\t (1, 4, 4, 16)\n","flatten output shape\t (1, 256)\n","dense output shape\t (1, 120)\n","dense_1 output shape\t (1, 84)\n","dense_2 output shape\t (1, 10)\n"]}],"source":["X = tf.random.uniform((1,28,28,1))\n","for layer in net.layers:\n","    X = layer(X)\n","    print(layer.name, 'output shape\\t', X.shape)"]},{"cell_type":"markdown","metadata":{},"source":["可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。\n","\n","\n","## 5.5.2 获取数据和训练模型\n","\n","下面我们来实验LeNet模型。实验中，我们仍然使用Fashion-MNIST作为训练数据集。"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:41.280097Z","iopub.status.busy":"2024-10-21T13:54:41.279178Z","iopub.status.idle":"2024-10-21T13:54:42.135374Z","shell.execute_reply":"2024-10-21T13:54:42.134564Z","shell.execute_reply.started":"2024-10-21T13:54:41.280056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}],"source":["from tensorflow.keras.datasets import fashion_mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:45.967032Z","iopub.status.busy":"2024-10-21T13:54:45.966124Z","iopub.status.idle":"2024-10-21T13:54:46.061954Z","shell.execute_reply":"2024-10-21T13:54:46.060914Z","shell.execute_reply.started":"2024-10-21T13:54:45.966988Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 28, 28, 1)\n"]}],"source":["train_images = tf.reshape(train_images, \n","                          (train_images.shape[0],\n","                           train_images.shape[1],\n","                           train_images.shape[2], 1))\n","print(train_images.shape)\n","\n","test_images = tf.reshape(test_images, \n","                         (test_images.shape[0],\n","                          test_images.shape[1],\n","                          test_images.shape[2], 1))\n"]},{"cell_type":"markdown","metadata":{},"source":["损失函数和训练算法依然采用交叉熵损失函数(cross entropy)和小批量随机梯度下降(SGD)"]},{"cell_type":"markdown","metadata":{},"source":["这段代码在配置神经网络的训练参数,我用中文为您解释主要内容:\n","\n","1. **优化器设置**:\n","- 使用随机梯度下降(SGD)优化器\n","- 学习率设为0.9\n","- 动量参数设为0\n","- 没有使用Nesterov加速梯度\n","\n","2. **模型编译配置**:\n","- 使用`compile()`方法配置模型训练参数\n","- 损失函数选择`sparse_categorical_crossentropy`(稀疏分类交叉熵),适用于整数标签的多分类问题\n","- 评估指标选择准确率(accuracy)\n","\n","这是训练深度学习模型前的标准配置步骤。值得注意的是:\n","- 学习率0.9相对较大,可能会导致训练不稳定\n","- 没有使用动量,这可能会使收敛速度较慢\n","- 这些参数设置比较基础,现代实践中可能会选择更复杂的优化器(如Adam)或添加正则化等技术\n","\n","SGD优化器中的momentum和nesterov参数：\n","\n","### Momentum (动量)\n","- **作用**：帮助优化器在训练过程中保持一定的\"惯性\"，可以加快收敛速度并避免陷入局部最小值\n","- **工作原理**：\n","  - 不仅考虑当前梯度，还会考虑之前累积的梯度方向\n","  - momentum=0.0 表示完全不使用动量\n","  - 通常设置为0.9左右会有较好效果\n","- **优势**：\n","  - 可以加快收敛速度\n","  - 有助于跳出局部最小值\n","  - 在梯度方向震荡时具有平滑效果\n","\n","### Nesterov (涅斯捷罗夫加速梯度)\n","- **作用**：是动量方法的一种改进版本\n","- **工作原理**：\n","  - nesterov=False 表示不使用这种改进\n","  - 当设置为True时，会先根据动量移动，然后再计算梯度\n","  - 相比普通动量方法，能提供更好的收敛性能\n","- **优势**：\n","  - 对梯度提供了一种\"前瞻性\"的修正\n","  - 通常比普通动量方法收敛更快\n","  - 在某些情况下能获得更好的训练效果\n","\n","### 建议的改进\n","当前代码中这两个参数都没有启用（momentum=0.0，nesterov=False），建议的改进方案：\n","```python\n","optimizer = SGD(\n","    learning_rate=0.9,\n","    momentum=0.9,    # 添加动量\n","    nesterov=True    # 启用Nesterov加速\n",")\n","```\n","\n","这样的设置通常能带来更好的训练效果和更快的收敛速度。"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:49.667303Z","iopub.status.busy":"2024-10-21T13:54:49.666389Z","iopub.status.idle":"2024-10-21T13:54:49.690827Z","shell.execute_reply":"2024-10-21T13:54:49.689380Z","shell.execute_reply.started":"2024-10-21T13:54:49.667236Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import SGD\n","optimizer = SGD(learning_rate=0.9, momentum=0.0, \n","                nesterov=False)\n","\n","net.compile(optimizer=optimizer,\n","            loss='sparse_categorical_crossentropy',\n","            metrics=['accuracy'])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:54:51.858320Z","iopub.status.busy":"2024-10-21T13:54:51.857914Z","iopub.status.idle":"2024-10-21T13:57:38.372873Z","shell.execute_reply":"2024-10-21T13:57:38.371870Z","shell.execute_reply.started":"2024-10-21T13:54:51.858280Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1729518892.520666     101 service.cc:145] XLA service 0x79c3300075e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","I0000 00:00:1729518892.520719     101 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","I0000 00:00:1729518892.520724     101 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m  93/1688\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0926 - loss: 2.5967"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1729518894.103559     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.2943 - loss: 1.8291 - val_accuracy: 0.7438 - val_loss: 0.6584\n","Epoch 2/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7590 - loss: 0.6113 - val_accuracy: 0.7287 - val_loss: 0.6689\n","Epoch 3/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7977 - loss: 0.5202 - val_accuracy: 0.8143 - val_loss: 0.4766\n","Epoch 4/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8211 - loss: 0.4662 - val_accuracy: 0.7977 - val_loss: 0.5110\n","Epoch 5/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4410 - val_accuracy: 0.8385 - val_loss: 0.4257\n","Epoch 6/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.4244 - val_accuracy: 0.8025 - val_loss: 0.5049\n","Epoch 7/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8383 - loss: 0.4193 - val_accuracy: 0.7983 - val_loss: 0.5245\n","Epoch 8/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8248 - loss: 0.4516 - val_accuracy: 0.8508 - val_loss: 0.4029\n","Epoch 9/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8448 - loss: 0.4049 - val_accuracy: 0.8525 - val_loss: 0.3886\n","Epoch 10/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8527 - loss: 0.3794 - val_accuracy: 0.8592 - val_loss: 0.3796\n","Epoch 11/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8527 - loss: 0.3857 - val_accuracy: 0.8182 - val_loss: 0.4531\n","Epoch 12/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8381 - loss: 0.4176 - val_accuracy: 0.8377 - val_loss: 0.4056\n","Epoch 13/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8508 - loss: 0.3908 - val_accuracy: 0.8005 - val_loss: 0.5160\n","Epoch 14/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8440 - loss: 0.4032 - val_accuracy: 0.8495 - val_loss: 0.3906\n","Epoch 15/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8557 - loss: 0.3783 - val_accuracy: 0.8395 - val_loss: 0.4137\n","Epoch 16/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.3921 - val_accuracy: 0.8453 - val_loss: 0.3999\n","Epoch 17/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8498 - loss: 0.3884 - val_accuracy: 0.8382 - val_loss: 0.4177\n","Epoch 18/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8480 - loss: 0.3899 - val_accuracy: 0.8618 - val_loss: 0.3626\n","Epoch 19/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8612 - loss: 0.3567 - val_accuracy: 0.8552 - val_loss: 0.3702\n","Epoch 20/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8671 - loss: 0.3452 - val_accuracy: 0.8480 - val_loss: 0.3784\n","Epoch 21/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.3595 - val_accuracy: 0.8560 - val_loss: 0.3836\n","Epoch 22/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3397 - val_accuracy: 0.8698 - val_loss: 0.3425\n","Epoch 23/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3152 - val_accuracy: 0.8615 - val_loss: 0.3744\n","Epoch 24/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8672 - loss: 0.3395 - val_accuracy: 0.8475 - val_loss: 0.3868\n","Epoch 25/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3535 - val_accuracy: 0.8443 - val_loss: 0.4040\n","Epoch 26/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8662 - loss: 0.3446 - val_accuracy: 0.8543 - val_loss: 0.3877\n","Epoch 27/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8649 - loss: 0.3463 - val_accuracy: 0.8492 - val_loss: 0.3836\n","Epoch 28/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8757 - loss: 0.3260 - val_accuracy: 0.8637 - val_loss: 0.3472\n","Epoch 29/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8737 - loss: 0.3260 - val_accuracy: 0.8597 - val_loss: 0.3771\n","Epoch 30/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3278 - val_accuracy: 0.8540 - val_loss: 0.3886\n","Epoch 31/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8654 - loss: 0.3442 - val_accuracy: 0.8565 - val_loss: 0.3648\n","Epoch 32/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8744 - loss: 0.3234 - val_accuracy: 0.8607 - val_loss: 0.3671\n","Epoch 33/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8806 - loss: 0.3038 - val_accuracy: 0.8697 - val_loss: 0.3386\n","Epoch 34/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8728 - loss: 0.3287 - val_accuracy: 0.8627 - val_loss: 0.3606\n","Epoch 35/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3226 - val_accuracy: 0.8608 - val_loss: 0.3658\n","Epoch 36/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3262 - val_accuracy: 0.8622 - val_loss: 0.3539\n","Epoch 37/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8757 - loss: 0.3197 - val_accuracy: 0.8640 - val_loss: 0.3547\n","Epoch 38/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.2993 - val_accuracy: 0.8642 - val_loss: 0.3523\n","Epoch 39/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.2986 - val_accuracy: 0.8752 - val_loss: 0.3322\n","Epoch 40/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.3132 - val_accuracy: 0.8580 - val_loss: 0.3679\n","Epoch 41/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8693 - loss: 0.3282 - val_accuracy: 0.8477 - val_loss: 0.3912\n","Epoch 42/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8770 - loss: 0.3146 - val_accuracy: 0.8613 - val_loss: 0.3667\n","Epoch 43/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8795 - loss: 0.3109 - val_accuracy: 0.8663 - val_loss: 0.3522\n","Epoch 44/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.3131 - val_accuracy: 0.8563 - val_loss: 0.3652\n","Epoch 45/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.2977 - val_accuracy: 0.8612 - val_loss: 0.3698\n","Epoch 46/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.2992 - val_accuracy: 0.8618 - val_loss: 0.3623\n","Epoch 47/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3122 - val_accuracy: 0.8663 - val_loss: 0.3531\n","Epoch 48/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.2995 - val_accuracy: 0.8648 - val_loss: 0.3598\n","Epoch 49/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8835 - loss: 0.2988 - val_accuracy: 0.8573 - val_loss: 0.3739\n","Epoch 50/50\n","\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8827 - loss: 0.2978 - val_accuracy: 0.8732 - val_loss: 0.3424\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x79c3c043ffa0>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["net.fit(train_images, train_labels, \n","        epochs=50, validation_split=0.1)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:57:59.487505Z","iopub.status.busy":"2024-10-21T13:57:59.486766Z","iopub.status.idle":"2024-10-21T13:57:59.522987Z","shell.execute_reply":"2024-10-21T13:57:59.522238Z","shell.execute_reply.started":"2024-10-21T13:57:59.487461Z"},"trusted":true},"outputs":[],"source":["# 保存模型权重数据\n","net.save_weights(\"5.5_lenet.weights.h5\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:58:09.992013Z","iopub.status.busy":"2024-10-21T13:58:09.991246Z","iopub.status.idle":"2024-10-21T13:58:10.487788Z","shell.execute_reply":"2024-10-21T13:58:10.486790Z","shell.execute_reply.started":"2024-10-21T13:58:09.991974Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 - 0s - 1ms/step - accuracy: 0.8586 - loss: 0.3684\n"]},{"data":{"text/plain":["[0.36839860677719116, 0.8586000204086304]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["net.evaluate(test_images, test_labels, verbose=2)"]},{"cell_type":"markdown","metadata":{},"source":["## 小结\n","\n","* 卷积神经网络就是含卷积层的网络。\n","* LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
