{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二维卷积层\n",
    "\n",
    "卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。本章中介绍的卷积神经网络均使用最常见的二维卷积层。它有高和宽两个空间维度，常用来处理图像数据。本节中，我们将介绍简单形式的二维卷积层的工作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 two dimentional cross-correlation\n",
    "\n",
    "虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。\n",
    "我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为$3 \\times 3$或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即$2 \\times 2$。图5.1中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$。\n",
    "\n",
    "![二维互相关运算](../img/correlation.svg)\n",
    "\n",
    "在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\\\\\n",
    "$$\n",
    "\n",
    "下面我们将上述过程实现在`corr2d`函数里。它接受输入数组`X`与核数组`K`，并输出数组`Y`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **函数定义**：\n",
    "```python\n",
    "def corr2d(X, K):\n",
    "```\n",
    "\n",
    "- X：输入矩阵（可以是图像）\n",
    "- K：卷积核（或称为滤波器）\n",
    "\n",
    "2. **输出初始化**：\n",
    "```python\n",
    "h, w = K.shape\n",
    "Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
    "```\n",
    "\n",
    "- 获取卷积核的高度h和宽度w\n",
    "- 创建输出矩阵Y，尺寸为：\n",
    "  - 高度 = X高度 - K高度 + 1\n",
    "  - 宽度 = X宽度 - K宽度 + 1\n",
    "- 初始化为全0矩阵\n",
    "\n",
    "3. **卷积计算过程**：\n",
    "```python\n",
    "for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        window_sum = tf.reduce_sum(X[i:i+h, j:j+w] * K)\n",
    "        result = tf.cast(window_sum, dtype=tf.float32)\n",
    "        Y[i,j].assign(result)\n",
    "```\n",
    "\n",
    "- 使用双重循环遍历输出矩阵的每个位置\n",
    "- 对于每个位置：\n",
    "  - 提取对应的输入窗口 X[i:i+h, j:j+w]\n",
    "  - 与卷积核K进行元素乘法\n",
    "  - 计算乘积的总和\n",
    "  - 将结果转换为float32类型\n",
    "  - 赋值给输出矩阵对应位置\n",
    "\n",
    "这个函数的特点：\n",
    "1. **手动实现**：\n",
    "   - 没有使用TensorFlow的内置卷积操作\n",
    "   - 清晰展示了卷积的计算过程\n",
    "\n",
    "2. **教学价值**：\n",
    "   - 帮助理解卷积运算的本质\n",
    "   - 展示了滑动窗口的工作方式\n",
    "\n",
    "3. **局限性**：\n",
    "   - 计算效率较低（使用循环）\n",
    "   - 没有考虑批量处理\n",
    "   - 没有实现填充和步幅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w +1)))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            # 计算卷积窗口内的元素乘积和\n",
    "            window_sum = tf.reduce_sum(X[i:i+h, j:j+w] * K)\n",
    "            # 将结果转换为float32类型\n",
    "            result = tf.cast(window_sum, dtype=tf.float32)\n",
    "            # 将计算结果赋值给Y[i,j]\n",
    "            Y[i,j].assign(result)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以构造图5.1中的输入数组`X`、核数组`K`来验证二维互相关运算的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[19., 25.],\n",
       "       [37., 43.]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[0,1,2], [3,4,5], [6,7,8]])\n",
    "K = tf.constant([[0,1], [2,3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Conv2d\n",
    "\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n",
    "\n",
    "下面基于`corr2d`函数来实现一个自定义的二维卷积层。在构造函数`__init__`里我们声明`weight`和`bias`这两个模型参数。前向计算函数`forward`则是直接调用`corr2d`函数再加上偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "这段代码定义了一个自定义的二维卷积层类，用于理解卷积层的实现原理。让我详细解释其结构和功能：\n",
    "\n",
    "1. **类定义**：\n",
    "```python\n",
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units  # 定义卷积层的输出通道数\n",
    "```\n",
    "\n",
    "\n",
    "- 继承自tf.keras.layers.Layer\n",
    "- 初始化时指定输出通道数\n",
    "- 这是一个教学用的简化实现\n",
    "\n",
    "2. **构建方法**：\n",
    "```python\n",
    "def build(self, kernel_size):\n",
    "    self.w = self.add_weight(\n",
    "        name='w',\n",
    "        shape=kernel_size,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    self.b = self.add_weight(\n",
    "        name='b',\n",
    "        shape=(1,),\n",
    "        initializer=tf.random_normal_initializer())\n",
    "```\n",
    "\n",
    "\n",
    "- 根据指定的kernel_size初始化卷积核\n",
    "- 创建两个可训练参数：\n",
    "  - w：卷积核权重，使用正态分布初始化\n",
    "  - b：偏置项，同样使用正态分布初始化\n",
    "  - 都是可训练的参数\n",
    "\n",
    "3. **前向传播方法**：\n",
    "```python\n",
    "def call(self, inputs):\n",
    "    return corr2d(inputs, self.w) + self.b\n",
    "```\n",
    "\n",
    "\n",
    "- 使用之前定义的corr2d函数执行卷积操作\n",
    "- 加上偏置项\n",
    "- 返回卷积结果\n",
    "\n",
    "这个类的特点：\n",
    "1. **教学目的**：\n",
    "   - 简化的卷积层实现\n",
    "   - 帮助理解卷积操作的基本原理\n",
    "\n",
    "2. **局限性**：\n",
    "   - 没有实现多通道输入\n",
    "   - 没有实现多卷积核\n",
    "   - 没有实现步幅和填充\n",
    "   - 使用了效率较低的corr2d函数\n",
    "\n",
    "3. **基本组成**：\n",
    "   - 可训练的卷积核\n",
    "   - 可训练的偏置项\n",
    "   - 基本的卷积运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该类在后续中没有被实际使用，仅仅是为了理解卷积层的实现原理\n",
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units  # 定义卷积层的输出通道数\n",
    "    \n",
    "    def build(self, kernel_size):\n",
    "        # 初始化卷积核权重\n",
    "        self.w = self.add_weight(\n",
    "            name='w',\n",
    "            shape=kernel_size,\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        \n",
    "        # 初始化偏置项\n",
    "        self.b = self.add_weight(\n",
    "            name='b',\n",
    "            shape=(1,),\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # 执行卷积操作并加上偏置项\n",
    "        return corr2d(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积窗口形状为$p \\times q$的卷积层称为$p \\times q$卷积层。同样，$p \\times q$卷积或$p \\times q$卷积核说明卷积核的高和宽分别为$p$和$q$。\n",
    "\n",
    "## 5.1.3 edge detection\n",
    "\n",
    "下面我们来看一个卷积层的简单应用：检测图像中物体的边缘，即找到像素变化的位置。首先我们构造一张$6\\times 8$的图像（即高和宽分别为6像素和8像素的图像）。它中间4列为黑（0），其余为白（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 8) dtype=float32, numpy=\n",
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable(tf.ones((6,8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:,2:6].shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们构造一个高和宽分别为1和2的卷积核`K`。当它与输入做互相关运算时，如果横向相邻元素相同，输出为0；否则输出为非0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.constant([[1,-1]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将输入`X`和我们设计的卷积核`K`做互相关运算。可以看出，我们将从白到黑的边缘和从黑到白的边缘分别检测成了1和-1。其余部分的输出全是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 7) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此，我们可以看出，卷积层可通过重复使用卷积核有效地表征局部空间。\n",
    "\n",
    "## 5.1.4 learn kernel by data\n",
    "\n",
    "最后我们来看一个例子，它使用物体边缘检测中的输入数据`X`和输出数据`Y`来学习我们构造的核数组`K`。我们首先构造一个卷积层，将其卷积核初始化成随机数组。接下来在每一次迭代中，我们使用平方误差来比较`Y`和卷积层的输出，然后计算梯度来更新权重。简单起见，这里的卷积层忽略了偏差。\n",
    "\n",
    "虽然我们之前构造了`Conv2D`类，但由于`corr2d`使用了对单个元素赋值（`[i, j]=`）的操作因而无法自动求梯度。下面我们使用tf.keras.layers提供的`Conv2D`类来实现这个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **输入数据重塑**：\n",
    "```python\n",
    "X = tf.reshape(X, (1,6,8,1))\n",
    "Y = tf.reshape(Y, (1,6,7,1))\n",
    "```\n",
    "\n",
    "\n",
    "- 将2D数据转换为4D格式\n",
    "- 维度含义：(批量大小, 高度, 宽度, 通道数)\n",
    "- X从(6,8)变为(1,6,8,1)\n",
    "- Y从(6,7)变为(1,6,7,1)\n",
    "- 这是因为卷积层需要4D输入格式\n",
    "\n",
    "2. **创建卷积层**：\n",
    "```python\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 2))\n",
    "```\n",
    "\n",
    "\n",
    "参数说明：\n",
    "- filters=1：输出通道数为1，即只生成一个特征图\n",
    "- kernel_size=(1,2)：使用1×2的卷积核\n",
    "- 默认使用线性激活函数（无激活函数）\n",
    "\n",
    "重要概念：\n",
    "1. **维度要求**：\n",
    "   - Conv2D层要求输入为4D张量\n",
    "   - 格式：(batch_size, height, width, channels)\n",
    "\n",
    "2. **卷积核参数**：\n",
    "   - 形状维度：(kernel_height, kernel_width, input_channels, filters)\n",
    "   - 这里是(1, 2, 1, 1)的卷积核\n",
    "\n",
    "3. **数据预处理**：\n",
    "   - 需要根据卷积层的要求调整输入数据维度\n",
    "   - 添加批量维度和通道维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将X的维度从(6, 8)调整为(1, 6, 8, 1)，以适应卷积层的输入要求\n",
    "X = tf.reshape(X, (1,6,8,1))\n",
    "# 将Y的维度从(6, 7)调整为(1, 6, 7, 1)，以适应卷积层的输出要求\n",
    "Y = tf.reshape(Y, (1,6,7,1))\n",
    "Y\n",
    "        \n",
    "# 创建一个2D卷积层\n",
    "# filters=1: 定义输出通道数为1,即只产生一个特征图\n",
    "# kernel_size=(1,2): 定义卷积核大小为1x2\n",
    "# 具体API见Keras库\n",
    "# GPT的Prompt链接：https://g.co/gemini/share/837a8138956a\n",
    "# Conv2D形状维度: (kernel_height, kernel_width, input_channels, filters)\n",
    "# 默认激活函数为linear\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "这段代码实现了卷积层参数的训练过程。让我详细解释其工作流程：\n",
    "\n",
    "1. **初始化**：\n",
    "```python\n",
    "Y_hat = conv2d(X)\n",
    "```\n",
    "\n",
    "- 使用卷积层对输入X进行初始预测\n",
    "\n",
    "2. **训练循环**：\n",
    "```python\n",
    "for i in range(100):\n",
    "```\n",
    "\n",
    "- 设置最大迭代次数为100\n",
    "- 使用梯度下降优化卷积层参数\n",
    "\n",
    "3. **前向传播和损失计算**：\n",
    "```python\n",
    "with tf.GradientTape() as g:\n",
    "    Y_hat = conv2d(X)\n",
    "    l = tf.reduce_sum((Y_hat - Y) ** 2)\n",
    "    if l < 1e-3:\n",
    "        break\n",
    "```\n",
    "\n",
    "- 使用GradientTape记录计算过程\n",
    "- 计算卷积层输出\n",
    "- 计算均方误差损失\n",
    "- 当损失小于阈值时提前结束训练\n",
    "\n",
    "4. **梯度计算和参数更新**：\n",
    "```python\n",
    "dl = g.gradient(l, conv2d.trainable_variables)\n",
    "lr = 3e-2\n",
    "conv2d.trainable_variables[0].assign_sub(lr * dl[0])\n",
    "```\n",
    "\n",
    "- 计算损失对参数的梯度\n",
    "- 设置学习率为0.03\n",
    "- 仅更新部分权重参数（避免损失发散）\n",
    "\n",
    "5. **训练监控**：\n",
    "```python\n",
    "if (i + 1) % 2 == 0:\n",
    "    print('批次 %d, 损失 %.3f' % (i + 1, l))\n",
    "```\n",
    "\n",
    "- 每两次迭代打印一次损失值\n",
    "- 监控训练进度\n",
    "\n",
    "重要说明：\n",
    "1. **参数更新策略**：\n",
    "   - 只更新部分权重参数\n",
    "   - 注释中说明更新所有参数会导致损失发散\n",
    "\n",
    "2. **提前停止条件**：\n",
    "   - 当损失小于1e-3时停止训练\n",
    "   - 防止过度训练\n",
    "\n",
    "3. **训练特点**：\n",
    "   - 使用简单的梯度下降\n",
    "   - 固定学习率\n",
    "   - 手动实现的训练循环\n",
    "\n",
    "这段代码展示了：\n",
    "- 如何训练卷积层的参数\n",
    "- 梯度下降的基本实现\n",
    "- 训练过程中的注意事项\n",
    "- 如何监控训练进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 2, 损失 20.170\n",
      "批次 4, 损失 3.611\n",
      "批次 6, 损失 0.699\n",
      "批次 8, 损失 0.155\n",
      "批次 10, 损失 0.042\n",
      "批次 12, 损失 0.013\n",
      "批次 14, 损失 0.005\n",
      "批次 16, 损失 0.002\n"
     ]
    }
   ],
   "source": [
    "# 初始化卷积层的输出\n",
    "Y_hat = conv2d(X)\n",
    "\n",
    "# 进行10次迭代训练\n",
    "for i in range(100):\n",
    "    # 使用GradientTape记录梯度\n",
    "    with tf.GradientTape() as g:\n",
    "        # 计算卷积层的输出\n",
    "        Y_hat = conv2d(X)\n",
    "        # 计算均方误差损失\n",
    "        l = tf.reduce_sum((Y_hat - Y) ** 2)\n",
    "        if l < 1e-3:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # 计算损失对卷积层参数的梯度\n",
    "    dl = g.gradient(l, conv2d.trainable_variables)\n",
    "    \n",
    "    # 设置学习率\n",
    "    lr = 3e-2\n",
    "    \n",
    "    # 反向传播，更新部分权重参数\n",
    "    conv2d.trainable_variables[0].assign_sub(lr * dl[0])\n",
    "    # 更新所有权重参数\n",
    "    # 实验表示更新全部权重会导致Loss发散\n",
    "    # 批次 2, 损失 58.790\n",
    "    # 批次 4, 损失 3209.656\n",
    "    # 批次 6, 损失 178068.516\n",
    "    # 批次 8, 损失 9879409.000\n",
    "    # 批次 10, 损失 548118784.000\n",
    "    # for j, grad in enumerate(dl):\n",
    "    #     conv2d.trainable_variables[j].assign_sub(lr * grad)\n",
    "    \n",
    "    # 每两次迭代打印一次损失\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print('批次 %d, 损失 %.3f' % (i + 1, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，10次迭代后误差已经降到了一个比较小的值。现在来看一下学习到的核数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重形状 (1, 2, 1, 1)\n",
      "偏置形状 (1,)\n",
      "激活函数名称: linear\n"
     ]
    }
   ],
   "source": [
    "print(\"权重形状\", conv2d.get_weights()[0].shape)\n",
    "print(\"偏置形状\", conv2d.get_weights()[1].shape)\n",
    "print(\"激活函数名称:\", conv2d.activation.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，学到的核数组与我们之前定义的核数组`K`较接近。\n",
    "\n",
    "## 互相关运算和卷积运算\n",
    "\n",
    "实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。\n",
    "\n",
    "那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出图5.1中的核数组。设其他条件不变，使用卷积运算学出的核数组即图5.1中的核数组按上下、左右翻转。也就是说，图5.1中的输入与学出的已翻转的核数组再做卷积运算时，依然得到图5.1中的输出。为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。\n",
    "\n",
    "\n",
    "## 特征图和感受野\n",
    "\n",
    "二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图5.1中形状为$2 \\times 2$的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。\n",
    "\n",
    "我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。当含义明确时，本书不对这两个术语做严格区分。\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。\n",
    "* 我们可以设计卷积核来检测图像中的边缘。\n",
    "* 我们可以通过数据来学习卷积核。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sonnet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
