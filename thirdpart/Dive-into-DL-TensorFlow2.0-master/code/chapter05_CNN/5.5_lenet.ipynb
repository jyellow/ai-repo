{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前可用的GPU设备: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"当前可用的GPU设备:\", gpu)  # 输出当前的GPU设备\n",
    "else:\n",
    "    print(\"没有检测到可用的GPU设备。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络（LeNet）\n",
    "\n",
    "在[“多层感知机的从零开始实现”](../chapter_deep-learning-basics/mlp-scratch.ipynb)一节里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。\n",
    "\n",
    "1. 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n",
    "2. 对于大尺寸的输入图像，使用全连接层容易导致模型过大。假设输入是高和宽均为$1,000$像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状也是$3,000,000\\times 256$：它占用了大约3 GB的内存或显存。这会带来过于复杂的模型和过高的存储开销。\n",
    "\n",
    "卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n",
    "\n",
    "卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet [1]。这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。\n",
    "\n",
    "## 5.5.1 LeNet模型\n",
    "\n",
    "LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n",
    "\n",
    "卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。\n",
    "\n",
    "卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n",
    "\n",
    "下面我们通过`Sequential`类来实现LeNet模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='sigmoid'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d output shape\t (1, 24, 24, 6)\n",
      "max_pooling2d output shape\t (1, 12, 12, 6)\n",
      "conv2d_1 output shape\t (1, 8, 8, 16)\n",
      "max_pooling2d_1 output shape\t (1, 4, 4, 16)\n",
      "flatten output shape\t (1, 256)\n",
      "dense output shape\t (1, 120)\n",
      "dense_1 output shape\t (1, 84)\n",
      "dense_2 output shape\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform((1,28,28,1))\n",
    "for layer in net.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。\n",
    "\n",
    "\n",
    "## 5.5.2 获取数据和训练模型\n",
    "\n",
    "下面我们来实验LeNet模型。实验中，我们仍然使用Fashion-MNIST作为训练数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = tf.reshape(train_images, (train_images.shape[0],train_images.shape[1],train_images.shape[2], 1))\n",
    "print(train_images.shape)\n",
    "\n",
    "test_images = tf.reshape(test_images, (test_images.shape[0],test_images.shape[1],test_images.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数和训练算法依然采用交叉熵损失函数(cross entropy)和小批量随机梯度下降(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.9, momentum=0.0, nesterov=False)\n",
    "\n",
    "net.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4917 - accuracy: 0.8088 - val_loss: 0.6210 - val_accuracy: 0.7648\n",
      "Epoch 2/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4789 - accuracy: 0.8143 - val_loss: 0.5218 - val_accuracy: 0.7963\n",
      "Epoch 3/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4726 - accuracy: 0.8161 - val_loss: 0.4547 - val_accuracy: 0.8262\n",
      "Epoch 4/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4735 - accuracy: 0.8141 - val_loss: 0.4611 - val_accuracy: 0.8120\n",
      "Epoch 5/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4513 - accuracy: 0.8206 - val_loss: 0.4456 - val_accuracy: 0.8225\n",
      "Epoch 6/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4292 - accuracy: 0.8338 - val_loss: 0.4517 - val_accuracy: 0.8227\n",
      "Epoch 7/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4214 - accuracy: 0.8357 - val_loss: 0.3999 - val_accuracy: 0.8448\n",
      "Epoch 8/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4023 - accuracy: 0.8444 - val_loss: 0.4655 - val_accuracy: 0.8102\n",
      "Epoch 9/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4384 - accuracy: 0.8297 - val_loss: 0.4569 - val_accuracy: 0.8215\n",
      "Epoch 10/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3960 - accuracy: 0.8469 - val_loss: 0.4004 - val_accuracy: 0.8485\n",
      "Epoch 11/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4092 - accuracy: 0.8413 - val_loss: 0.4194 - val_accuracy: 0.8418\n",
      "Epoch 12/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4136 - accuracy: 0.8380 - val_loss: 0.4623 - val_accuracy: 0.8135\n",
      "Epoch 13/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4190 - accuracy: 0.8374 - val_loss: 0.4826 - val_accuracy: 0.8060\n",
      "Epoch 14/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4041 - accuracy: 0.8422 - val_loss: 0.4182 - val_accuracy: 0.8325\n",
      "Epoch 15/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3802 - accuracy: 0.8513 - val_loss: 0.3840 - val_accuracy: 0.8452\n",
      "Epoch 16/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3752 - accuracy: 0.8531 - val_loss: 0.3870 - val_accuracy: 0.8448\n",
      "Epoch 17/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3625 - accuracy: 0.8593 - val_loss: 0.3656 - val_accuracy: 0.8602\n",
      "Epoch 18/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3671 - accuracy: 0.8562 - val_loss: 0.4641 - val_accuracy: 0.8175\n",
      "Epoch 19/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4147 - accuracy: 0.8372 - val_loss: 0.3867 - val_accuracy: 0.8480\n",
      "Epoch 20/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3770 - accuracy: 0.8535 - val_loss: 0.3915 - val_accuracy: 0.8483\n",
      "Epoch 21/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3671 - accuracy: 0.8573 - val_loss: 0.3863 - val_accuracy: 0.8463\n",
      "Epoch 22/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3542 - accuracy: 0.8620 - val_loss: 0.3770 - val_accuracy: 0.8555\n",
      "Epoch 23/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3502 - accuracy: 0.8658 - val_loss: 0.3657 - val_accuracy: 0.8615\n",
      "Epoch 24/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3348 - accuracy: 0.8721 - val_loss: 0.3651 - val_accuracy: 0.8590\n",
      "Epoch 25/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3208 - accuracy: 0.8762 - val_loss: 0.3350 - val_accuracy: 0.8707\n",
      "Epoch 26/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3414 - accuracy: 0.8684 - val_loss: 0.4054 - val_accuracy: 0.8430\n",
      "Epoch 27/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3352 - accuracy: 0.8692 - val_loss: 0.3360 - val_accuracy: 0.8712\n",
      "Epoch 28/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3261 - accuracy: 0.8744 - val_loss: 0.3734 - val_accuracy: 0.8585\n",
      "Epoch 29/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3115 - accuracy: 0.8792 - val_loss: 0.3490 - val_accuracy: 0.8710\n",
      "Epoch 30/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3218 - accuracy: 0.8776 - val_loss: 0.3621 - val_accuracy: 0.8648\n",
      "Epoch 31/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3225 - accuracy: 0.8755 - val_loss: 0.3585 - val_accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3410 - accuracy: 0.8678 - val_loss: 0.3610 - val_accuracy: 0.8652\n",
      "Epoch 33/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3167 - accuracy: 0.8779 - val_loss: 0.3884 - val_accuracy: 0.8523\n",
      "Epoch 34/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3046 - accuracy: 0.8835 - val_loss: 0.3402 - val_accuracy: 0.8768\n",
      "Epoch 35/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2974 - accuracy: 0.8846 - val_loss: 0.3459 - val_accuracy: 0.8692\n",
      "Epoch 36/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2995 - accuracy: 0.8844 - val_loss: 0.3279 - val_accuracy: 0.8725\n",
      "Epoch 37/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2933 - accuracy: 0.8880 - val_loss: 0.3684 - val_accuracy: 0.8572\n",
      "Epoch 38/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2839 - accuracy: 0.8910 - val_loss: 0.3730 - val_accuracy: 0.8600\n",
      "Epoch 39/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2900 - accuracy: 0.8878 - val_loss: 0.3400 - val_accuracy: 0.8768\n",
      "Epoch 40/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2902 - accuracy: 0.8875 - val_loss: 0.3967 - val_accuracy: 0.8498\n",
      "Epoch 41/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2957 - accuracy: 0.8855 - val_loss: 0.3437 - val_accuracy: 0.8703\n",
      "Epoch 42/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2776 - accuracy: 0.8927 - val_loss: 0.3409 - val_accuracy: 0.8748\n",
      "Epoch 43/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2776 - accuracy: 0.8946 - val_loss: 0.3471 - val_accuracy: 0.8668\n",
      "Epoch 44/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2988 - accuracy: 0.8850 - val_loss: 0.3657 - val_accuracy: 0.8615\n",
      "Epoch 45/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2985 - accuracy: 0.8835 - val_loss: 0.3569 - val_accuracy: 0.8657\n",
      "Epoch 46/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2960 - accuracy: 0.8855 - val_loss: 0.3565 - val_accuracy: 0.8695\n",
      "Epoch 47/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3172 - accuracy: 0.8770 - val_loss: 0.3589 - val_accuracy: 0.8677\n",
      "Epoch 48/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2896 - accuracy: 0.8871 - val_loss: 0.3555 - val_accuracy: 0.8698\n",
      "Epoch 49/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2804 - accuracy: 0.8903 - val_loss: 0.3564 - val_accuracy: 0.8672\n",
      "Epoch 50/50\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2791 - accuracy: 0.8909 - val_loss: 0.3685 - val_accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181ddc84310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_images, train_labels, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型权重数据\n",
    "net.save_weights(\"5.5_lenet.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.3817 - accuracy: 0.8558 - 448ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38168200850486755, 0.8557999730110168]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 卷积神经网络就是含卷积层的网络。\n",
    "* LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
