{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 使用重复元素的网络（VGG）\n","\n","AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。\n","\n","本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group [1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。\n","\n","## VGG块\n","\n","VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为$3\\times 3$的卷积层后接上一个步幅为2、窗口形状为$2\\times 2$的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用`vgg_block`函数来实现这个基础的VGG块，它可以指定卷积层的数量`num_convs`和输出通道数`num_channels`。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:05.953172Z","iopub.status.busy":"2024-10-22T02:41:05.952732Z","iopub.status.idle":"2024-10-22T02:41:21.310356Z","shell.execute_reply":"2024-10-22T02:41:21.309064Z","shell.execute_reply.started":"2024-10-22T02:41:05.953125Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","for gpu in tf.config.experimental.list_physical_devices('GPU'):\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"markdown","metadata":{},"source":["1. **函数定义**：\n","   - 函数名为 `vgg_block`\n","   - 接收两个参数：\n","     - `num_convs`：卷积层的数量\n","     - `num_channels`：卷积层的输出通道数\n","\n","2. **块的结构**：\n","   - 使用 Sequential 模型创建一个顺序的网络块\n","   - 包含两种主要层：\n","     - 多个卷积层\n","     - 一个最大池化层\n","\n","3. **卷积层特点**：\n","   - 使用 3×3 的卷积核\n","   - 使用 'same' 填充保持输入输出尺寸一致\n","   - 使用 ReLU 激活函数\n","   - 重复 num_convs 次\n","\n","4. **池化层特点**：\n","   - 使用 2×2 的池化窗口\n","   - 步长为 2\n","   - 用于降低特征图的空间维度\n","\n","5. **实际应用**：\n","   - 这个块是 VGG 网络的基本构建单元\n","   - 通过调整 num_convs 和 num_channels 可以构建不同深度和宽度的网络层\n","\n","这种模块化的设计使得 VGG 网络结构更加清晰和易于理解，同时也便于构建不同版本的 VGG 网络。"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:21.312448Z","iopub.status.busy":"2024-10-22T02:41:21.311794Z","iopub.status.idle":"2024-10-22T02:41:21.320422Z","shell.execute_reply":"2024-10-22T02:41:21.318601Z","shell.execute_reply.started":"2024-10-22T02:41:21.312404Z"},"trusted":true},"outputs":[],"source":["def vgg_block(num_convs, num_channels):\n","    blk = tf.keras.models.Sequential()\n","    for _ in range(num_convs):\n","        blk.add(tf.keras.layers.Conv2D(\n","            num_channels,\n","            kernel_size=3,\n","            padding='same',\n","            activation='relu'\n","        ))\n","    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","    return blk"]},{"cell_type":"markdown","metadata":{},"source":["## VGG网络\n","\n","与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个`vgg_block`，其超参数由变量`conv_arch`定义。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则跟AlexNet中的一样。\n","\n","现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","\n","这段代码实现了VGG网络的完整构建过程，我详细解释如下：\n","\n","### 1. 网络架构配置\n","```python\n","conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n","```\n","这个元组定义了VGG网络的卷积层架构：\n","- 包含5个VGG块\n","- 每个元组包含两个数值：(卷积层数量, 输出通道数)\n","- 具体配置：\n","  - 第一块：1层卷积，64通道\n","  - 第二块：1层卷积，128通道\n","  - 第三块：2层卷积，256通道\n","  - 第四块：2层卷积，512通道\n","  - 第五块：2层卷积，512通道\n","\n","### 2. VGG网络构建函数\n","```python\n","def vgg(conv_arch)\n","```\n","\n","#### 网络构建过程\n","1. **卷积部分**：\n","   - 使用Sequential模型作为基础容器\n","   - 遍历`conv_arch`中的配置\n","   - 根据配置添加VGG块（使用之前定义的`vgg_block`函数）\n","\n","2. **全连接部分**：\n","添加一个包含以下层的Sequential模型：\n","   - Flatten层：将卷积特征展平\n","   - 两个4096单元的全连接层：\n","     - 使用ReLU激活\n","     - 每个后面跟随0.5概率的Dropout\n","   - 输出层：\n","     - 10个单元（对应类别数）\n","     - 使用sigmoid激活函数\n","\n","### 技术特点\n","1. **模块化设计**：\n","   - 使用`conv_arch`参数化网络结构\n","   - 便于调整网络深度和宽度\n","\n","2. **规范化结构**：\n","   - 遵循VGG的标准设计原则\n","   - 通道数逐层递增\n","   - 使用统一的全连接层配置\n","\n","3. **防过拟合措施**：\n","   - 在全连接层使用Dropout\n","   - 使用标准化的激活函数\n","\n","这个实现展示了VGG网络的典型特征：深度架构、规则的结构设计和有效的正则化策略。"]},{"cell_type":"markdown","metadata":{},"source":["详细计算VGG网络每一层的参数数量：\n","\n","### 1. 卷积块部分\n","\n","**第一块 (1层, 64通道)**\n","- 卷积层: (3×3×1×64) + 64 = 640\n","- 参数总计: 640\n","\n","**第二块 (1层, 128通道)**\n","- 卷积层: (3×3×64×128) + 128 = 73,856\n","- 参数总计: 73,856\n","\n","**第三块 (2层, 256通道)**\n","- 第一层: (3×3×128×256) + 256 = 295,168\n","- 第二层: (3×3×256×256) + 256 = 590,080\n","- 参数总计: 885,248\n","\n","**第四块 (2层, 512通道)**\n","- 第一层: (3×3×256×512) + 512 = 1,180,160\n","- 第二层: (3×3×512×512) + 512 = 2,359,808\n","- 参数总计: 3,539,968\n","\n","**第五块 (2层, 512通道)**\n","- 第一层: (3×3×512×512) + 512 = 2,359,808\n","- 第二层: (3×3×512×512) + 512 = 2,359,808\n","- 参数总计: 4,719,616\n","\n","### 2. 全连接部分\n","\n","**展平层**\n","- 参数数量: 0\n","\n","**第一个全连接层**\n","- 输入维度: 7×7×512 = 25,088\n","- 参数: (25,088×4096) + 4096 = 102,764,544\n","\n","**第一个Dropout层**\n","- 参数数量: 0\n","\n","**第二个全连接层**\n","- 参数: (4096×4096) + 4096 = 16,781,312\n","\n","**第二个Dropout层**\n","- 参数数量: 0\n","\n","**输出层**\n","- 参数: (4096×10) + 10 = 40,970\n","\n","### 总参数统计\n","- 卷积层部分: 9,219,328\n","- 全连接层部分: 119,586,826\n","- 总参数量: 128,806,154\n","\n","### 特点分析\n","1. 参数主要集中在全连接层（约93%）\n","2. 第一个全连接层占据了最多参数（约80%）\n","3. 卷积层参数相对较少，但提取了关键特征\n","4. 这种参数分布也解释了为什么后续的网络架构（如ResNet）倾向于减少全连接层的使用"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:21.323185Z","iopub.status.busy":"2024-10-22T02:41:21.322721Z","iopub.status.idle":"2024-10-22T02:41:21.338555Z","shell.execute_reply":"2024-10-22T02:41:21.337168Z","shell.execute_reply.started":"2024-10-22T02:41:21.323142Z"},"trusted":true},"outputs":[],"source":["conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"]},{"cell_type":"markdown","metadata":{},"source":["下面我们实现VGG-11。"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:21.342682Z","iopub.status.busy":"2024-10-22T02:41:21.342200Z","iopub.status.idle":"2024-10-22T02:41:21.506572Z","shell.execute_reply":"2024-10-22T02:41:21.505271Z","shell.execute_reply.started":"2024-10-22T02:41:21.342620Z"},"trusted":true},"outputs":[],"source":["def vgg(conv_arch):\n","    net = tf.keras.models.Sequential()\n","    for (num_convs, num_channels) in conv_arch:\n","        net.add(vgg_block(num_convs, num_channels))\n","    net.add(tf.keras.models.Sequential([\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(4096, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(4096, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(10, activation='sigmoid')]))\n","    return net\n","\n","net = vgg(conv_arch)"]},{"cell_type":"markdown","metadata":{},"source":["下面构造一个高和宽均为224的单通道数据样本来观察每一层的输出形状。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:21.508260Z","iopub.status.busy":"2024-10-22T02:41:21.507906Z","iopub.status.idle":"2024-10-22T02:41:23.016513Z","shell.execute_reply":"2024-10-22T02:41:23.015262Z","shell.execute_reply.started":"2024-10-22T02:41:21.508223Z"},"trusted":true},"outputs":[],"source":["X = tf.random.uniform((1,224,224,1))\n","for blk in net.layers:\n","    X = blk(X)\n","    print(blk.name, 'output shape:\\t', X.shape)"]},{"cell_type":"markdown","metadata":{},"source":["可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"]},{"cell_type":"markdown","metadata":{},"source":["## 获取数据和训练模型"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:23.018554Z","iopub.status.busy":"2024-10-22T02:41:23.018124Z","iopub.status.idle":"2024-10-22T02:41:23.057742Z","shell.execute_reply":"2024-10-22T02:41:23.056578Z","shell.execute_reply.started":"2024-10-22T02:41:23.018513Z"},"trusted":true},"outputs":[],"source":["# 因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练。\n","# ratio = 4\n","# small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n","# net = vgg(small_conv_arch)"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","这是一个用于处理Fashion-MNIST数据集的数据加载器实现，我详细解释其结构和功能：\n","\n","### 1. 类定义与初始化\n","**DataLoader类**包含以下主要组件：\n","\n","#### 初始化函数 (`__init__`)\n","- 加载Fashion-MNIST数据集\n","- 数据预处理：\n","  - 图像归一化到[0,1]区间\n","  - 添加通道维度\n","  - 转换数据类型（float32和int32）\n","- 记录训练集和测试集的样本数量\n","\n","#### 数据批次获取方法\n","1. **训练数据获取** (`get_batch_train`)\n","   - 随机采样指定批次大小的数据\n","   - 将图像调整为224×224大小（适配VGG网络）\n","   - 返回处理后的图像和标签\n","\n","2. **测试数据获取** (`get_batch_test`)\n","   - 功能类似训练数据获取\n","   - 用于模型评估阶段\n","\n","#### 数据可视化方法 (`dataset_explorate`)\n","- 展示数据集的前10个样本\n","- 使用matplotlib创建2×5的图像网格\n","- 显示每个样本的图像和对应类别标签\n","- 类别包括：T恤、裤子、套衫等10种服装类型\n","\n","### 2. 可视化配置\n","- 使用自定义的深度学习样式\n","- 设置图像大小为10×10\n","- 去除坐标轴显示\n","- 使用灰度颜色映射\n","\n","### 3. 使用示例\n","- 设置批次大小为128\n","- 创建DataLoader实例\n","- 获取一个训练批次\n","- 打印数据形状\n","- 显示样本可视化\n","\n","### 技术特点\n","1. **数据预处理**：\n","   - 标准化\n","   - 维度处理\n","   - 类型转换\n","\n","2. **灵活性**：\n","   - 支持批量数据获取\n","   - 可调整图像大小\n","   - 随机采样策略\n","\n","3. **可视化功能**：\n","   - 直观展示数据集内容\n","   - 清晰的标签显示\n","   - 专业的图像布局\n","\n","这个实现展示了深度学习中数据处理的标准实践，包括数据预处理、批处理和可视化等关键环节。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:23.059413Z","iopub.status.busy":"2024-10-22T02:41:23.059077Z","iopub.status.idle":"2024-10-22T02:41:24.028119Z","shell.execute_reply":"2024-10-22T02:41:24.026890Z","shell.execute_reply.started":"2024-10-22T02:41:23.059378Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('./deeplearning.mplstyle')\n","# plt.style.use('seaborn-darkgrid')  # 设置matplotlib样式\n","class DataLoader():\n","    def __init__(self):\n","        fashion_mnist = tf.keras.datasets.fashion_mnist\n","        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = fashion_mnist.load_data()\n","        self.train_images = np.expand_dims(self.train_images.astype(np.float32)/255.0,axis=-1)\n","        self.test_images = np.expand_dims(self.test_images.astype(np.float32)/255.0,axis=-1)\n","        self.train_labels = self.train_labels.astype(np.int32)\n","        self.test_labels = self.test_labels.astype(np.int32)\n","        self.num_train, self.num_test = self.train_images.shape[0], self.test_images.shape[0]\n","        \n","    def get_batch_train(self, batch_size):\n","        index = np.random.randint(0, np.shape(self.train_images)[0], batch_size)\n","        #need to resize images to (224,224)\n","        resized_images = tf.image.resize_with_pad(self.train_images[index],224,224,)\n","        return resized_images.numpy(), self.train_labels[index]\n","    \n","    def get_batch_test(self, batch_size):\n","        index = np.random.randint(0, np.shape(self.test_images)[0], batch_size)\n","        #need to resize images to (224,224)\n","        resized_images = tf.image.resize_with_pad(self.test_images[index],224,224,)\n","        return resized_images.numpy(), self.test_labels[index]\n","\n","    def dataset_explorate(self):\n","        \"\"\"展示数据集前10条数据\"\"\"\n","        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  # 标签名称列表\n","        plt.figure(figsize=(10, 10))\n","        for i in range(10):\n","            plt.subplot(2, 5, i + 1)  # 创建2行5列的子图\n","            plt.imshow(self.train_images[i].squeeze(), cmap='gray')  # 显示图像，去掉通道维度\n","            plt.title(f'Label: {class_names[self.train_labels[i]]}')  # 显示标签\n","            plt.axis('off')  # 不显示坐标轴\n","        plt.show()\n","\n","batch_size = 128\n","dataLoader = DataLoader()\n","x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n","print(\"x_batch shape:\",x_batch.shape,\"y_batch shape:\", y_batch.shape)\n","dataLoader.dataset_explorate()"]},{"cell_type":"markdown","metadata":{},"source":["除了使用了稍大些的学习率，模型训练过程与上一节的AlexNet中的类似。\n","\n","注：这里省略了训练过程的输出，如果您需要进行训练，请执行train_vgg()函数"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","\n","这段代码实现了VGG网络的训练配置和训练过程，我详细解释如下：\n","\n","### 1. 训练函数定义\n","`train_vgg()` 函数实现了完整的训练流程：\n","- 设置训练轮数(epoch)为5\n","- 计算每轮的迭代次数（总样本数/批次大小）\n","- 使用双层循环进行训练\n","- 定期保存模型权重\n","\n","### 2. 模型编译配置\n","使用 `compile()` 方法设置训练参数：\n","\n","**优化器配置**：\n","- 使用SGD（随机梯度下降）优化器\n","- 学习率：0.05\n","- 动量：0.0（未使用动量）\n","- 不使用Nesterov加速\n","\n","**训练参数**：\n","- 损失函数：sparse_categorical_crossentropy（适用于整数标签的多分类问题）\n","- 评估指标：准确率（accuracy）\n","\n","### 3. 训练过程\n","1. **批次训练**：\n","   - 获取一个批次的训练数据\n","   - 使用`fit()`方法进行训练\n","\n","2. **权重保存策略**：\n","   - 每20个批次保存一次权重\n","   - 在每轮最后一个批次也保存权重\n","   - 保存格式为h5文件\n","\n","### 4. 代码特点\n","1. **模型保护**：\n","   - 定期保存权重防止训练中断\n","   - 保留了加载预训练权重的选项（当前被注释）\n","\n","2. **训练配置**：\n","   - 使用基础的SGD优化器\n","   - 采用标准的分类损失函数\n","   - 包含准确率监控\n","\n","3. **测试代码**：\n","   - 包含单批次训练测试\n","   - 完整训练函数被注释（# train_vgg()）\n","\n","这个实现展示了深度学习模型训练的标准流程，包括模型配置、训练循环和权重保存等关键环节。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T02:41:24.030241Z","iopub.status.busy":"2024-10-22T02:41:24.029862Z"},"trusted":true},"outputs":[],"source":["def train_vgg():\n","#     net.load_weights(\"5.7_vgg_weights.h5\")\n","    epoch = 5\n","    num_iter = dataLoader.num_train//batch_size\n","    for e in range(epoch):\n","        for n in range(num_iter):\n","            x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n","            net.fit(x_batch, y_batch)\n","            # 每20个批次保存一次，或者是最后一个批次时保存\n","            if n % 20 == 0 or n == num_iter - 1:\n","                net.save_weights(\"5.7_vgg.weights.h5\")\n","                \n","\n","net.compile(\n","    optimizer=tf.keras.optimizers.SGD(\n","        learning_rate=0.05,\n","        momentum=0.0,\n","        nesterov=False\n","    ),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n","net.fit(x_batch, y_batch)\n","# train_vgg()"]},{"cell_type":"markdown","metadata":{},"source":["我们将训练好的参数读入，然后取测试数据计算测试准确率"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["net.load_weights(\"5.7_vgg.weights.h5\")\n","\n","x_test, y_test = dataLoader.get_batch_test(2000)\n","net.evaluate(x_test, y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{},"source":["## 小结\n","\n","* VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
