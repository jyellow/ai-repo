{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "EAePZMkOL2zd",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "# 3.13 丢弃法\n",
        "除了前一节介绍的权重衰减以外，深度学习模型常常使用丢弃法（dropout）[1] 来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o2vTywd1L2zf"
      },
      "source": [
        "根据丢弃法的定义，我们可以很容易地实现它。下面的dropout函数将以drop_prob的概率丢弃NDArray输入X中的元素。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FnjbKb_CQAa"
      },
      "source": [
        "## 3.13.2. 从零开始实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hr6WElJSL2zg",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这是一个实现神经网络dropout功能的函数，它的主要目的是通过随机关闭一些神经元来防止过拟合。具体工作过程如下：\n",
        "\n",
        "1. **输入检查**：\n",
        "   - 函数接收两个参数：神经元的输出值X和需要丢弃的概率drop_prob\n",
        "   - 首先检查丢弃概率是否在0到1之间，这是一个合理性检查\n",
        "\n",
        "2. **特殊情况处理**：\n",
        "   - 计算保留概率keep_prob（等于1减去丢弃概率）\n",
        "   - 如果保留概率是0（即丢弃概率是1），就直接返回一个全是0的张量\n",
        "   - 这种情况意味着所有神经元都被关闭\n",
        "\n",
        "3. **随机选择要保留的神经元**：\n",
        "   - 生成一个和输入X形状相同的随机数矩阵，每个数都在0到1之间\n",
        "   - 将这些随机数与保留概率比较，小于保留概率的位置标记为True\n",
        "   - 这样就创建了一个随机的布尔掩码，决定哪些神经元保留，哪些丢弃\n",
        "\n",
        "4. **输出结果计算**：\n",
        "   - 将布尔掩码转换为数值（True变成1，False变成0）\n",
        "   - 将保留的神经元输出值除以保留概率进行放大\n",
        "   - 这样做是为了保持整体输出的期望值不变\n",
        "   - 被丢弃的神经元输出为0，保留的神经元输出会相应放大\n",
        "\n",
        "这个过程就像是在打牌时随机扣掉一些牌，但为了保持游戏平衡，剩下的牌的分值会相应提高。这种随机丢弃的方式可以防止神经网络过分依赖某些特定的神经元，从而提高模型的泛化能力。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "76kX5DVN6Gsk"
      },
      "outputs": [],
      "source": [
        "def dropout(X, drop_prob):\n",
        "    # 确保dropout概率在[0,1]范围内\n",
        "    assert 0 <= drop_prob <= 1\n",
        "    \n",
        "    # 计算保留概率\n",
        "    keep_prob = 1 - drop_prob\n",
        "    \n",
        "    # 如果保留概率为0（即丢弃概率为1）\n",
        "    # 则返回全0张量，相当于完全丢弃\n",
        "    if keep_prob == 0:\n",
        "        return tf.zeros_like(X)\n",
        "    \n",
        "    # 生成随机掩码（mask）\n",
        "    # 1. tf.random.uniform生成[0,1)之间的均匀分布随机数\n",
        "    # 2. 将随机数与keep_prob比较，生成布尔型掩码\n",
        "    # 3. 小于keep_prob的位置为True，表示保留\n",
        "    mask = tf.random.uniform(\n",
        "        shape=X.shape, \n",
        "        minval=0, \n",
        "        maxval=1) < keep_prob\n",
        "    \n",
        "    # 应用dropout\n",
        "    # 1. 将布尔型mask转换为float32类型（True->1.0, False->0.0）\n",
        "    # 2. 将输入X转换为float32类型\n",
        "    # 3. 对保留的值进行缩放（除以keep_prob），以保持期望值不变\n",
        "    return tf.cast(mask, dtype=tf.float32) * tf.cast(X, dtype=tf.float32) / keep_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "zumVlyqoL2zi",
        "outputId": "6b6357ea-c114-417c-d8a1-dc3c884f1ca2",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
              "array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
              "       [ 8.,  9., 10., 11., 12., 13., 14., 15.]], dtype=float32)>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = tf.reshape(tf.range(0, 16), shape=(2, 8))\n",
        "dropout(X, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "EHx1SBztL2zl",
        "outputId": "c7770ec5-3e32-42b2-8953-9ecde4aecc4e",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
              "array([[ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.],\n",
              "       [ 0.,  0., 20., 22., 24.,  0., 28., 30.]], dtype=float32)>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dropout(X, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "FPs6YPXiL2zn",
        "outputId": "435340ab-3bb7-47a4-9bae-8949e63e6194",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dropout(X, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpx4rjmZL2zp"
      },
      "source": [
        "### 3.13.2.1. 定义模型参数¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "定义一个三层神经网络的参数结构：\n",
        "\n",
        "1. **网络结构定义**：\n",
        "   - 输入层有784个神经元（处理28×28的图像）\n",
        "   - 两个隐藏层，每层都有256个神经元\n",
        "   - 输出层有10个神经元（做10分类任务）\n",
        "\n",
        "2. **第一层参数**：\n",
        "   - W1是连接输入层和第一隐藏层的权重矩阵（784×256）\n",
        "   - W1是连接输入层和第一隐藏层的权重矩阵（784×256）\n",
        "   - 使用标准差为0.01的正态分布初始化权重\n",
        "   - b1是第一隐藏层的偏置项，初始化为0\n",
        "\n",
        "3. **第二层参数**：\n",
        "   - W2是连接两个隐藏层的权重矩阵（256×256）\n",
        "   - 使用标准差为0.1的正态分布初始化权重\n",
        "   - b2是第二隐藏层的偏置项，初始化为0\n",
        "\n",
        "4. **输出层参数**：\n",
        "   - W3是连接第二隐藏层和输出层的权重矩阵（256×10）\n",
        "   - 使用截断正态分布初始化权重，避免过大的权重值\n",
        "   - b3是输出层的偏置项，初始化为0\n",
        "\n",
        "5. **参数集合**：\n",
        "   - 将所有参数放入一个列表中，方便统一管理和更新\n",
        "\n",
        "这个网络结构是一个典型的多层感知机（MLP），用于图像分类任务。通过不同的初始化方式（正态分布、截断正态分布）和不同的标准差值，可以帮助网络更好地收敛和学习。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lgno77Th8-UF"
      },
      "outputs": [],
      "source": [
        "# 定义神经网络的基本参数\n",
        "num_inputs, num_outputs = 784, 10  # 输入层784个神经元(28x28图像),输出层10个神经元(10分类)\n",
        "num_hiddens1, num_hiddens2 = 256, 256  # 两个隐藏层,每层256个神经元\n",
        "\n",
        "# 第一层参数初始化\n",
        "# 权重W1使用标准差0.01的正态分布初始化,shape为(784,256)\n",
        "W1 = tf.Variable(tf.random.normal(stddev=0.01, shape=(num_inputs, num_hiddens1)))\n",
        "b1 = tf.Variable(tf.zeros(num_hiddens1))  # 偏置b1初始化为0\n",
        "\n",
        "# 第二层参数初始化\n",
        "# 权重W2使用标准差0.1的正态分布初始化,shape为(256,256) \n",
        "W2 = tf.Variable(tf.random.normal(stddev=0.1, shape=(num_hiddens1, num_hiddens2)))\n",
        "b2 = tf.Variable(tf.zeros(num_hiddens2))  # 偏置b2初始化为0\n",
        "\n",
        "# 输出层参数初始化\n",
        "# 权重W3使用截断正态分布初始化,shape为(256,10)\n",
        "W3 = tf.Variable(tf.random.truncated_normal(stddev=0.01, shape=(num_hiddens2, num_outputs)))\n",
        "b3 = tf.Variable(tf.zeros(num_outputs))  # 偏置b3初始化为0\n",
        "\n",
        "# 将所有参数放入列表统一管理\n",
        "params = [W1, b1, W2, b2, W3, b3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "klSup1CwL2zs"
      },
      "source": [
        "### 3.13.2.2. 定义模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H3-N6TtAL2zs"
      },
      "outputs": [],
      "source": [
        "# 定义两个隐藏层的dropout比例\n",
        "# 第一层dropout较少(20%)以保留更多底层特征 \n",
        "# 第二层dropout较多(50%)以防止过拟合\n",
        "drop_prob1, drop_prob2 = 0.2, 0.5\n",
        "\n",
        "def net(X, is_training=False):\n",
        "    # 将输入X重塑为二维张量\n",
        "    # 第一维(-1)为批量大小（自动计算），第二维为输入特征数(784)\n",
        "    X = tf.reshape(X, shape=(-1, num_inputs))\n",
        "    \n",
        "    # 第一个隐藏层\n",
        "    # 1. 线性变换：tf.matmul(X, W1) + b1\n",
        "    # 2. ReLU激活函数\n",
        "    H1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "    if is_training:  # 只在训练模式下应用dropout\n",
        "        H1 = dropout(H1, drop_prob1)  # 对第一层输出使用20%的dropout\n",
        "    \n",
        "    # 第二个隐藏层\n",
        "    # 1. 线性变换：tf.matmul(H1, W2) + b2\n",
        "    # 2. ReLU激活函数\n",
        "    H2 = tf.nn.relu(tf.matmul(H1, W2) + b2)\n",
        "    if is_training:  # 只在训练模式下应用dropout\n",
        "        H2 = dropout(H2, drop_prob2)  # 对第二层输出使用50%的dropout\n",
        "    \n",
        "    # 输出层\n",
        "    # 1. 线性变换：tf.matmul(H2, W3) + b3\n",
        "    # 2. softmax激活函数用于多分类\n",
        "    return tf.math.softmax(tf.matmul(H2, W3) + b3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sKdzqexCCCce"
      },
      "source": [
        "### 3.13.2.3. 训练和测试模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "Od5pa1veDmr3",
        "outputId": "0d8081da-5cba-41df-d454-419886568202"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "batch_size=256\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = tf.cast(x_train, tf.float32) / 255 \n",
        "x_test = tf.cast(x_test,tf.float32) / 255\n",
        "train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aDtX08s3D_n4"
      },
      "outputs": [],
      "source": [
        "# 描述,对于tensorflow2中，比较的双方必须类型都是int型，所以要将输出和标签都转为int型\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    for _, (X, y) in enumerate(data_iter):\n",
        "        y = tf.cast(y,dtype=tf.int64)\n",
        "        acc_sum += np.sum(tf.cast(tf.argmax(net(X), axis=1), dtype=tf.int64) == y)\n",
        "        n += y.shape[0]\n",
        "    return acc_sum / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DNmVaxxmCoPk"
      },
      "outputs": [],
      "source": [
        "def train_ch3(net: callable, \n",
        "              train_iter: tf.data.Dataset, \n",
        "              test_iter: tf.data.Dataset, \n",
        "              loss: callable, \n",
        "              num_epochs: int, \n",
        "              batch_size: int,\n",
        "              params: list = None, \n",
        "              lr: float = None, \n",
        "              trainer: keras.optimizers.Optimizer = None):\n",
        "    \"\"\"训练模型的函数\n",
        "    \n",
        "    Args:\n",
        "        net: 神经网络模型\n",
        "        train_iter: 训练数据集迭代器\n",
        "        test_iter: 测试数据集迭代器  \n",
        "        loss: 损失函数\n",
        "        num_epochs: 训练轮数\n",
        "        batch_size: 批量大小\n",
        "        params: 模型参数列表,默认为None\n",
        "        lr: 学习率,默认为None\n",
        "        trainer: 优化器,默认为None\n",
        "    \"\"\"\n",
        "    global sample_grads\n",
        "    for epoch in range(num_epochs):  # 训练所需的迭代次数\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0  # 初始化训练损失之和、训练准确率之和、样本数\n",
        "        for X, y in train_iter:  # 遍历训练数据集\n",
        "            with tf.GradientTape() as tape:  # 记录梯度\n",
        "                y_hat = net(X, is_training=True)  # 前向传播,启用dropout\n",
        "                # 计算损失,将标签转换为one-hot编码\n",
        "                l = tf.reduce_sum(loss(y_hat, tf.one_hot(y, depth=10, axis=-1, dtype=tf.float32)))\n",
        "            \n",
        "            grads = tape.gradient(l, params)  # 计算梯度\n",
        "            if trainer is None:  # 如果没有定义优化器\n",
        "                sample_grads = grads  # 保存梯度用于观察\n",
        "                # 使用梯度下降优化参数\n",
        "                params[0].assign_sub(grads[0] * lr)\n",
        "                params[1].assign_sub(grads[1] * lr)\n",
        "            else:\n",
        "                trainer.apply_gradients(zip(grads, params))  # 使用优化器更新参数\n",
        "\n",
        "            y = tf.cast(y, dtype=tf.float32)  # 将标签转换为float32类型\n",
        "            train_l_sum += l.numpy()  # 累加训练损失\n",
        "            # 计算并累加训练准确率\n",
        "            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=1) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()\n",
        "            n += y.shape[0]  # 累加训练样本数\n",
        "        test_acc = evaluate_accuracy(test_iter, net)  # 评估测试集准确率\n",
        "        # 打印每轮训练的信息\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "6mDplouQL2zu",
        "outputId": "f66eb9e4-93c0-4c6b-c994-3cd65f2d1014",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0355, train acc 0.557, test acc 0.630\n",
            "epoch 2, loss 0.0269, train acc 0.619, test acc 0.636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-28 09:45:57.403376: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3, loss 0.0253, train acc 0.635, test acc 0.656\n",
            "epoch 4, loss 0.0239, train acc 0.664, test acc 0.688\n",
            "epoch 5, loss 0.0226, train acc 0.687, test acc 0.720\n"
          ]
        }
      ],
      "source": [
        "# 设置训练参数\n",
        "num_epochs = 5\n",
        "lr = 0.5 \n",
        "batch_size = 256\n",
        "\n",
        "# 定义损失函数\n",
        "loss = tf.losses.CategoricalCrossentropy()\n",
        "\n",
        "# 训练模型\n",
        "train_ch3(net, train_iter, test_iter, loss, \n",
        "          num_epochs, batch_size, params, lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "aL4Q9QAZL2zw"
      },
      "source": [
        "## 3.13.3 简洁实现\n",
        "在Tensorflow2.0中，我们只需要在全连接层后添加Dropout层并指定丢弃概率。在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时（即model.eval()后），Dropout层并不发挥作用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "colab_type": "code",
        "id": "2wqwvxNSL2zw",
        "outputId": "2cf5e1a7-483f-4e32-fb20-1a5d9c084cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6719 - loss: 0.9287 - val_accuracy: 0.8379 - val_loss: 0.4474\n",
            "Epoch 2/5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8418 - loss: 0.4429 - val_accuracy: 0.8577 - val_loss: 0.3885\n",
            "Epoch 3/5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8595 - loss: 0.3919 - val_accuracy: 0.8668 - val_loss: 0.3721\n",
            "Epoch 4/5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8706 - loss: 0.3573 - val_accuracy: 0.8690 - val_loss: 0.3685\n",
            "Epoch 5/5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8747 - loss: 0.3413 - val_accuracy: 0.8685 - val_loss: 0.3684\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x15140f500>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    keras.layers.Dense(256, activation='relu'), \n",
        "    Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=5,\n",
        "    batch_size=256,\n",
        "    validation_data=(x_test, y_test),\n",
        "    validation_freq=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "rhoznDzbL2zy"
      },
      "source": [
        "小结\n",
        "我们可以通过使用丢弃法应对过拟合。\n",
        "丢弃法只在训练模型时使用。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sFHN8HzQL2zz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3.13_dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
