{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1315631",
   "metadata": {
    "papermill": {
     "duration": 0.004766,
     "end_time": "2024-10-22T06:51:41.113780",
     "exception": false,
     "start_time": "2024-10-22T06:51:41.109014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 网络中的网络（NiN）\n",
    "\n",
    "前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
    "\n",
    "\n",
    "## NiN块\n",
    "\n",
    "我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在[“多输入通道和多输出通道”](channels.ipynb)一节里介绍的$1\\times 1$卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用$1\\times 1$卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。\n",
    "\n",
    "![左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部](../img/nin.svg)\n",
    "\n",
    "NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的$1\\times 1$卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbb3df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:51:41.123577Z",
     "iopub.status.busy": "2024-10-22T06:51:41.123234Z",
     "iopub.status.idle": "2024-10-22T06:52:00.094261Z",
     "shell.execute_reply": "2024-10-22T06:52:00.093196Z"
    },
    "papermill": {
     "duration": 18.978559,
     "end_time": "2024-10-22T06:52:00.096747",
     "exception": false,
     "start_time": "2024-10-22T06:51:41.118188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f1c86",
   "metadata": {},
   "source": [
    "网络中的网络(Network in Network, NiN)架构的基本构建块\n",
    "\n",
    "1. 函数定义：\n",
    "- 函数名为 `nin_block`，用于创建一个 NiN 块\n",
    "- 接收四个参数：\n",
    "  - num_channels：输出通道数\n",
    "  - kernel_size：卷积核大小\n",
    "  - strides：步长\n",
    "  - padding：填充方式\n",
    "\n",
    "2. 架构组成：\n",
    "每个 NiN 块包含三个连续的卷积层：\n",
    "\n",
    "- 第一层是标准卷积层：\n",
    "  - 可配置卷积核大小、步长和填充方式\n",
    "  - 使用 ReLU 激活函数\n",
    "  - 输出通道数由 num_channels 参数指定\n",
    "\n",
    "- 第二层是 1×1 卷积层：\n",
    "  - 使用 1×1 的卷积核\n",
    "  - 保持与第一层相同的输出通道数\n",
    "  - 使用 ReLU 激活函数\n",
    "  - 功能类似于全连接层\n",
    "\n",
    "- 第三层也是 1×1 卷积层：\n",
    "  - 配置与第二层相同\n",
    "  - 进一步增强网络的非线性表达能力\n",
    "\n",
    "3. 创新特点：\n",
    "- 使用 1×1 卷积替代传统的全连接层\n",
    "- 保持了特征的空间信息\n",
    "- 减少了参数数量\n",
    "- 增强了模型的非线性表达能力\n",
    "\n",
    "4. 实现方式：\n",
    "- 使用 TensorFlow 的 Keras Sequential API 构建\n",
    "- 采用模块化设计，便于在更大的网络中重复使用\n",
    "\n",
    "## 每一层的作用\n",
    "\n",
    "### 第一层：标准卷积层\n",
    "```python\n",
    "blk.add(Conv2D(num_channels, kernel_size,\n",
    "               strides=strides, padding=padding,\n",
    "               activation='relu'))\n",
    "```\n",
    "作用：\n",
    "1. 特征提取：\n",
    "   - 通过可自定义大小的卷积核提取空间特征\n",
    "   - 可以检测边缘、纹理等底层特征\n",
    "\n",
    "2. 参数灵活性：\n",
    "   - kernel_size：可调整卷积核大小\n",
    "   - strides：控制特征图缩放\n",
    "   - padding：控制输出大小\n",
    "\n",
    "3. 非线性变换：\n",
    "   - 使用 ReLU 激活引入非线性\n",
    "   - 提高模型表达能力\n",
    "\n",
    "### 第二层：1×1 卷积层\n",
    "```python\n",
    "blk.add(Conv2D(num_channels, kernel_size=1,\n",
    "               activation='relu'))\n",
    "```\n",
    "作用：\n",
    "1. 跨通道信息整合：\n",
    "   - 在不同通道间进行特征组合\n",
    "   - 相当于每个像素位置的全连接层\n",
    "\n",
    "2. 降维作用：\n",
    "   - 可以减少参数数量\n",
    "   - 控制模型复杂度\n",
    "\n",
    "3. 增加非线性：\n",
    "   - 通过 ReLU 引入额外的非线性变换\n",
    "   - 增强特征的表达能力\n",
    "\n",
    "### 第三层：1×1 卷积层\n",
    "```python\n",
    "blk.add(Conv2D(num_channels, kernel_size=1, \n",
    "               activation='relu'))\n",
    "```\n",
    "作用：\n",
    "1. 深度特征提取：\n",
    "   - 进一步提取更抽象的特征\n",
    "   - 增强模型的表示能力\n",
    "\n",
    "2. 模型正则化：\n",
    "   - 多层结构帮助防止过拟合\n",
    "   - 提高模型泛化能力\n",
    "\n",
    "3. 特征重组合：\n",
    "   - 对第二层的输出进行再次整合\n",
    "   - 产生更高层次的特征表示\n",
    "\n",
    "### 整体架构优势：\n",
    "\n",
    "1. 参数效率：\n",
    "   - 1×1 卷积大大减少了参数数量\n",
    "   - 相比传统全连接层更加高效\n",
    "\n",
    "2. 空间信息保持：\n",
    "   - 保留了特征的空间结构\n",
    "   - 有利于后续层的处理\n",
    "\n",
    "3. 多尺度特征：\n",
    "   - 通过多层架构捕获不同层次特征\n",
    "   - 提高模型的表达能力\n",
    "\n",
    "4. 计算效率：\n",
    "   - 1×1 卷积计算量小\n",
    "   - 适合在资源受限环境使用\n",
    "\n",
    "\n",
    "## 计算每一层神经网络的权重参数数量\n",
    "\n",
    "\n",
    "### NiN块第一层 - 标准卷积层\n",
    "```python\n",
    "Conv2D(num_channels, kernel_size, strides=strides, padding=padding, activation='relu')\n",
    "```\n",
    "参数计算公式:\n",
    "```\n",
    "参数数量 = (kernel_height × kernel_width × input_channels × output_channels) + output_channels\n",
    "```\n",
    "- kernel_height × kernel_width: 卷积核大小\n",
    "- input_channels: 输入通道数  \n",
    "- output_channels: 输出通道数(num_channels)\n",
    "- +output_channels: 偏置项\n",
    "\n",
    "### NiN块第二层 - 1×1卷积层  \n",
    "```python\n",
    "Conv2D(num_channels, kernel_size=1, activation='relu')\n",
    "```\n",
    "参数计算公式:\n",
    "```\n",
    "参数数量 = (1 × 1 × input_channels × output_channels) + output_channels\n",
    "```\n",
    "- 1×1: 卷积核大小为1×1\n",
    "- input_channels: 等于第一层的输出通道数(num_channels)\n",
    "- output_channels: 输出通道数(num_channels) \n",
    "- +output_channels: 偏置项\n",
    "\n",
    "### NiN块第三层 - 1×1卷积层\n",
    "```python\n",
    "Conv2D(num_channels, kernel_size=1, activation='relu')  \n",
    "```\n",
    "参数计算与第二层相同:\n",
    "```\n",
    "参数数量 = (1 × 1 × input_channels × output_channels) + output_channels\n",
    "```\n",
    "\n",
    "### 完整示例计算\n",
    "以第一个NiN块为例:\n",
    "```python\n",
    "net.add(nin_block(96, kernel_size=11, strides=4, padding='valid'))\n",
    "```\n",
    "\n",
    "第一层参数:\n",
    "- 卷积核: 11×11\n",
    "- 输入通道: 1(灰度图像)\n",
    "- 输出通道: 96\n",
    "```\n",
    "参数数量 = (11 × 11 × 1 × 96) + 96 = 11,616\n",
    "```\n",
    "\n",
    "第二层参数:\n",
    "- 卷积核: 1×1 \n",
    "- 输入通道: 96\n",
    "- 输出通道: 96\n",
    "```\n",
    "参数数量 = (1 × 1 × 96 × 96) + 96 = 9,312\n",
    "```\n",
    "\n",
    "第三层参数:\n",
    "- 与第二层相同\n",
    "```\n",
    "参数数量 = (1 × 1 × 96 × 96) + 96 = 9,312\n",
    "```\n",
    "\n",
    "第一个NiN块总参数:\n",
    "```\n",
    "总参数 = 11,616 + 9,312 + 9,312 = 30,240\n",
    "```\n",
    "\n",
    "同理可以计算其他NiN块:\n",
    "\n",
    "第二个NiN块(256输出通道):\n",
    "```python\n",
    "net.add(nin_block(256, kernel_size=5, strides=1, padding='same'))\n",
    "```\n",
    "- 第一层: (5 × 5 × 96 × 256) + 256 = 614,656\n",
    "- 第二层: (1 × 1 × 256 × 256) + 256 = 65,792  \n",
    "- 第三层: (1 × 1 × 256 × 256) + 256 = 65,792\n",
    "- 总参数: 746,240\n",
    "\n",
    "第三个NiN块(384输出通道):\n",
    "```python\n",
    "net.add(nin_block(384, kernel_size=3, strides=1, padding='same'))  \n",
    "```\n",
    "- 第一层: (3 × 3 × 256 × 384) + 384 = 885,120\n",
    "- 第二层: (1 × 1 × 384 × 384) + 384 = 147,840\n",
    "- 第三层: (1 × 1 × 384 × 384) + 384 = 147,840  \n",
    "- 总参数: 1,180,800\n",
    "\n",
    "通过这种计算,我们可以看到:\n",
    "1. 参数量主要集中在第一个卷积层\n",
    "2. 1×1卷积层大大减少了参数数量\n",
    "3. 随着通道数增加,参数量也显著增加\n",
    "\n",
    "这种设计在保持模型表达能力的同时,有效控制了参数数量,是一个比较高效的架构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa5a4ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:52:00.107135Z",
     "iopub.status.busy": "2024-10-22T06:52:00.106603Z",
     "iopub.status.idle": "2024-10-22T06:52:00.192515Z",
     "shell.execute_reply": "2024-10-22T06:52:00.191616Z"
    },
    "papermill": {
     "duration": 0.093431,
     "end_time": "2024-10-22T06:52:00.194684",
     "exception": false,
     "start_time": "2024-10-22T06:52:00.101253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    创建一个网络中的网络(NiN)块。\n",
    "\n",
    "    参数:\n",
    "    num_channels (int): 卷积层的输出通道数\n",
    "    kernel_size (int 或 tuple): 卷积核的大小\n",
    "    strides (int 或 tuple): 卷积步长\n",
    "    padding (str): 填充方式，'valid' 或 'same'\n",
    "\n",
    "    返回:\n",
    "    tf.keras.models.Sequential: NiN块的顺序模型\n",
    "    \"\"\"\n",
    "    # 创建一个顺序模型作为NiN块\n",
    "    blk = Sequential()\n",
    "    \n",
    "    # 添加第一个卷积层，可自定义参数\n",
    "    blk.add(Conv2D(num_channels, kernel_size,\n",
    "                   strides=strides, padding=padding,\n",
    "                   activation='relu'))\n",
    "    \n",
    "    # 添加第二个1x1卷积层，相当于全连接层\n",
    "    blk.add(Conv2D(num_channels, kernel_size=1,\n",
    "                   activation='relu'))\n",
    "    \n",
    "    # 添加第三个1x1卷积层，相当于全连接层\n",
    "    blk.add(Conv2D(num_channels, kernel_size=1, \n",
    "                   activation='relu'))\n",
    "    \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30082c4",
   "metadata": {
    "papermill": {
     "duration": 0.004189,
     "end_time": "2024-10-22T06:52:00.203423",
     "exception": false,
     "start_time": "2024-10-22T06:52:00.199234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NiN模型\n",
    "\n",
    "NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为$11\\times 11$、$5\\times 5$和$3\\times 3$的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为$3\\times 3$的最大池化层。\n",
    "\n",
    "除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31708ce9",
   "metadata": {},
   "source": [
    "### 1. 第一阶段：初始特征提取\n",
    "```python\n",
    "# 第一个NiN块\n",
    "net.add(nin_block(96, kernel_size=11, strides=4, padding='valid'))\n",
    "# 最大池化层\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 使用大尺寸卷积核(11×11)进行初始特征提取\n",
    "- 输出96个通道的特征图\n",
    "- 通过最大池化压缩特征图尺寸，保留重要特征\n",
    "\n",
    "### 2. 第二阶段：中层特征提取\n",
    "```python\n",
    "# 第二个NiN块\n",
    "net.add(nin_block(256, kernel_size=5, strides=1, padding='same'))\n",
    "# 最大池化层\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 使用中等尺寸卷积核(5×5)提取更复杂的特征\n",
    "- 增加到256个通道，提升特征表达能力\n",
    "- 再次使用池化层降维\n",
    "\n",
    "### 3. 第三阶段：高层特征提取\n",
    "```python\n",
    "# 第三个NiN块\n",
    "net.add(nin_block(384, kernel_size=3, strides=1, padding='same'))\n",
    "# 最大池化层\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 使用小尺寸卷积核(3×3)提取高级特征\n",
    "- 进一步增加到384个通道\n",
    "- 继续使用池化层降维\n",
    "\n",
    "### 4. 防止过拟合\n",
    "```python\n",
    "# Dropout层\n",
    "net.add(Dropout(0.5))\n",
    "```\n",
    "- 添加Dropout层随机丢弃50%的神经元\n",
    "- 有效防止模型过拟合\n",
    "\n",
    "### 5. 分类阶段\n",
    "```python\n",
    "# 最后一个NiN块\n",
    "net.add(nin_block(10, kernel_size=3, strides=1, padding='same'))\n",
    "# 全局平均池化层\n",
    "net.add(GlobalAveragePooling2D())\n",
    "# 展平层\n",
    "net.add(Flatten())\n",
    "```\n",
    "- 最后的NiN块将通道数减少到类别数(10)\n",
    "- 使用全局平均池化替代全连接层\n",
    "- 最终将特征展平为一维向量用于分类\n",
    "\n",
    "### 架构特点：\n",
    "1. 渐进式特征提取：\n",
    "   - 从大卷积核到小卷积核\n",
    "   - 从少通道到多通道\n",
    "   \n",
    "2. 多层次降维：\n",
    "   - 使用步长卷积和池化层逐步降维\n",
    "   - 保留重要特征信息\n",
    "\n",
    "3. 创新设计：\n",
    "   - 使用NiN块替代传统卷积层\n",
    "   - 用全局平均池化替代全连接层\n",
    "   \n",
    "4. 防过拟合措施：\n",
    "   - 使用Dropout层\n",
    "   - 采用全局平均池化减少参数\n",
    "\n",
    "## 计算每一层的权重数量\n",
    "\n",
    "### 1. 第一个NiN块\n",
    "输入：224×224×1 (灰度图像)\n",
    "```python\n",
    "net.add(nin_block(96, kernel_size=11, strides=4, padding='valid'))\n",
    "```\n",
    "第一层卷积(11×11):\n",
    "- 参数 = (11×11×1×96) + 96 = 11,616\n",
    "\n",
    "第二层卷积(1×1):\n",
    "- 参数 = (1×1×96×96) + 96 = 9,312\n",
    "\n",
    "第三层卷积(1×1):  \n",
    "- 参数 = (1×1×96×96) + 96 = 9,312\n",
    "\n",
    "第一个NiN块总参数：30,240\n",
    "\n",
    "### 2. 最大池化层\n",
    "```python\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 参数数量：0 (池化层没有可训练参数)\n",
    "\n",
    "### 3. 第二个NiN块\n",
    "输入通道从96变为256\n",
    "```python\n",
    "net.add(nin_block(256, kernel_size=5, strides=1, padding='same'))\n",
    "```\n",
    "第一层卷积(5×5):\n",
    "- 参数 = (5×5×96×256) + 256 = 614,656\n",
    "\n",
    "第二层卷积(1×1):\n",
    "- 参数 = (1×1×256×256) + 256 = 65,792\n",
    "\n",
    "第三层卷积(1×1):\n",
    "- 参数 = (1×1×256×256) + 256 = 65,792\n",
    "\n",
    "第二个NiN块总参数：746,240\n",
    "\n",
    "### 4. 最大池化层\n",
    "```python\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 参数数量：0\n",
    "\n",
    "### 5. 第三个NiN块\n",
    "输入通道从256变为384\n",
    "```python\n",
    "net.add(nin_block(384, kernel_size=3, strides=1, padding='same'))\n",
    "```\n",
    "第一层卷积(3×3):\n",
    "- 参数 = (3×3×256×384) + 384 = 885,120\n",
    "\n",
    "第二层卷积(1×1):\n",
    "- 参数 = (1×1×384×384) + 384 = 147,840\n",
    "\n",
    "第三层卷积(1×1):\n",
    "- 参数 = (1×1×384×384) + 384 = 147,840\n",
    "\n",
    "第三个NiN块总参数：1,180,800\n",
    "\n",
    "### 6. 最大池化层\n",
    "```python\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "```\n",
    "- 参数数量：0\n",
    "\n",
    "### 7. Dropout层\n",
    "```python\n",
    "net.add(Dropout(0.5))\n",
    "```\n",
    "- 参数数量：0\n",
    "\n",
    "### 8. 第四个NiN块(分类层)\n",
    "输入通道从384变为10\n",
    "```python\n",
    "net.add(nin_block(10, kernel_size=3, strides=1, padding='same'))\n",
    "```\n",
    "第一层卷积(3×3):\n",
    "- 参数 = (3×3×384×10) + 10 = 34,570\n",
    "\n",
    "第二层卷积(1×1):\n",
    "- 参数 = (1×1×10×10) + 10 = 110\n",
    "\n",
    "第三层卷积(1×1):\n",
    "- 参数 = (1×1×10×10) + 10 = 110\n",
    "\n",
    "第四个NiN块总参数：34,790\n",
    "\n",
    "### 9. 全局平均池化层\n",
    "```python\n",
    "net.add(GlobalAveragePooling2D())\n",
    "```\n",
    "- 参数数量：0\n",
    "\n",
    "### 10. 展平层\n",
    "```python\n",
    "net.add(Flatten())\n",
    "```\n",
    "- 参数数量：0\n",
    "\n",
    "### 总结\n",
    "总参数数量 = 30,240 + 746,240 + 1,180,800 + 34,790 = 1,992,070\n",
    "\n",
    "主要观察：\n",
    "1. 参数主要集中在中间的NiN块\n",
    "2. 第三个NiN块参数最多，占比约59%\n",
    "3. 后面的层参数反而减少，这得益于使用全局平均池化\n",
    "4. 通过这种设计，相比传统CNN大大减少了参数数量\n",
    "\n",
    "这种参数分布显示了NiN网络的高效性：在保持强大特征提取能力的同时，通过精心的架构设计控制了模型的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ceb717",
   "metadata": {},
   "source": [
    "## 平均池化和Tensorflow API介绍\n",
    "### 1. 平均池化(Average Pooling)的基本概念\n",
    "\n",
    "平均池化是一种降采样操作，它的工作原理是：\n",
    "```\n",
    "# 示例：2x2平均池化\n",
    "输入特征图片段：\n",
    "[[1, 2],\n",
    " [3, 4]]\n",
    "\n",
    "输出：(1 + 2 + 3 + 4) / 4 = 2.5\n",
    "```\n",
    "\n",
    "主要特点：\n",
    "1. 数据压缩：\n",
    "   - 将一个区域的值平均化为一个值\n",
    "   - 减少特征图的空间维度\n",
    "\n",
    "2. 特征提取：\n",
    "   - 保留区域的平均特征\n",
    "   - 对噪声有平滑作用\n",
    "\n",
    "3. 参数无关：\n",
    "   - 不需要训练参数\n",
    "   - 计算过程固定\n",
    "\n",
    "### 2. GlobalAveragePooling2D详解\n",
    "\n",
    "#### 2.1 API定义\n",
    "```python\n",
    "tf.keras.layers.GlobalAveragePooling2D(\n",
    "    data_format=None,\n",
    "    keepdims=False,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2.2 主要参数\n",
    "- data_format：数据格式\n",
    "  - 'channels_last'：(batch, height, width, channels)\n",
    "  - 'channels_first'：(batch, channels, height, width)\n",
    "  \n",
    "- keepdims：是否保持输出的维度\n",
    "  - False：压缩维度\n",
    "  - True：保持维度，但值为1\n",
    "\n",
    "#### 2.3 工作原理\n",
    "```python\n",
    "# 示例：\n",
    "输入形状：(batch_size, height, width, channels)\n",
    "输出形状：(batch_size, channels)\n",
    "\n",
    "# 具体计算过程\n",
    "对每个通道：\n",
    "1. 计算该通道所有像素的平均值\n",
    "2. 得到一个标量值\n",
    "```\n",
    "\n",
    "#### 2.4 代码示例\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 创建一个简单的测试数据\n",
    "# 形状：(1, 4, 4, 2) - 1个样本，4x4大小，2个通道\n",
    "input_data = tf.random.normal([1, 4, 4, 2])\n",
    "\n",
    "# 应用全局平均池化\n",
    "global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "output = global_pool(input_data)\n",
    "\n",
    "print(\"输入形状:\", input_data.shape)  # (1, 4, 4, 2)\n",
    "print(\"输出形状:\", output.shape)      # (1, 2)\n",
    "```\n",
    "\n",
    "### 3. GlobalAveragePooling2D的优势\n",
    "\n",
    "1. 减少参数数量：\n",
    "2. 防止过拟合：\n",
    "   - 没有需要训练的参数\n",
    "   - 降低了模型复杂度\n",
    "\n",
    "3. 保持空间信息：\n",
    "   - 考虑了整个特征图的信息\n",
    "   - 提供了全局的特征表示\n",
    "\n",
    "4. 空间不变性：\n",
    "   - 对输入大小的变化更加鲁棒\n",
    "   - 提高模型泛化能力\n",
    "\n",
    "### 4. 实际应用示例\n",
    "\n",
    "```python\n",
    "# 在NiN网络中的应用\n",
    "model = tf.keras.Sequential([\n",
    "    # ... 前面的卷积层 ...\n",
    "    tf.keras.layers.Conv2D(num_classes, 1, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    # 直接得到类别预测\n",
    "])\n",
    "\n",
    "# 可视化中间结果\n",
    "class VisualizeModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        print(\"卷积后形状:\", x.shape)\n",
    "        x = self.gap(x)\n",
    "        print(\"全局平均池化后形状:\", x.shape)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### 5. 性能考虑\n",
    "\n",
    "1. 计算效率：\n",
    "   - 简单的平均操作\n",
    "   - 计算速度快\n",
    "\n",
    "2. 内存效率：\n",
    "   - 不存储参数\n",
    "   - 内存占用小\n",
    "\n",
    "3. 梯度传播：\n",
    "   - 梯度计算简单\n",
    "   - 有助于训练稳定性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f62369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:52:00.213419Z",
     "iopub.status.busy": "2024-10-22T06:52:00.213108Z",
     "iopub.status.idle": "2024-10-22T06:52:00.391186Z",
     "shell.execute_reply": "2024-10-22T06:52:00.390197Z"
    },
    "papermill": {
     "duration": 0.18569,
     "end_time": "2024-10-22T06:52:00.393531",
     "exception": false,
     "start_time": "2024-10-22T06:52:00.207841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D, Dropout, GlobalAveragePooling2D, Flatten\n",
    "\n",
    "net = Sequential()\n",
    "# 第一个NiN块，96个输出通道，11x11卷积核，步长4\n",
    "net.add(nin_block(96, kernel_size=11, strides=4, padding='valid'))\n",
    "# 最大池化层，3x3窗口，步长2\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "# 第二个NiN块，256个输出通道，5x5卷积核，步长1\n",
    "net.add(nin_block(256, kernel_size=5, strides=1, padding='same'))\n",
    "# 最大池化层，3x3窗口，步长2\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "# 第三个NiN块，384个输出通道，3x3卷积核，步长1\n",
    "net.add(nin_block(384, kernel_size=3, strides=1, padding='same'))\n",
    "# 最大池化层，3x3窗口，步长2\n",
    "net.add(MaxPool2D(pool_size=3, strides=2))\n",
    "# Dropout层，防止过拟合\n",
    "net.add(Dropout(0.5))\n",
    "# 最后一个NiN块，10个输出通道（对应10个类别），3x3卷积核，步长1\n",
    "net.add(nin_block(10, kernel_size=3, strides=1, padding='same'))\n",
    "# 全局平均池化层，将每个通道的特征图平均成一个值\n",
    "net.add(GlobalAveragePooling2D())\n",
    "# 展平层，将结果转换为一维向量\n",
    "net.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54a786",
   "metadata": {
    "papermill": {
     "duration": 0.004754,
     "end_time": "2024-10-22T06:52:00.404042",
     "exception": false,
     "start_time": "2024-10-22T06:52:00.399288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "下面构造一个高和宽均为224的单通道数据样本来观察每一层的输出形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d21ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:52:00.414681Z",
     "iopub.status.busy": "2024-10-22T06:52:00.413797Z",
     "iopub.status.idle": "2024-10-22T06:52:02.513067Z",
     "shell.execute_reply": "2024-10-22T06:52:02.511893Z"
    },
    "papermill": {
     "duration": 2.106759,
     "end_time": "2024-10-22T06:52:02.515233",
     "exception": false,
     "start_time": "2024-10-22T06:52:00.408474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = tf.random.uniform((1,224,224,1))\n",
    "for blk in net.layers:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b5e9c",
   "metadata": {
    "papermill": {
     "duration": 0.004551,
     "end_time": "2024-10-22T06:52:02.524590",
     "exception": false,
     "start_time": "2024-10-22T06:52:02.520039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 获取数据和训练模型\n",
    "\n",
    "我们依然使用Fashion-MNIST数据集来训练模型。NiN的训练与AlexNet和VGG的类似，但这里使用的学习率更大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:52:02.535356Z",
     "iopub.status.busy": "2024-10-22T06:52:02.534631Z",
     "iopub.status.idle": "2024-10-22T06:52:10.064751Z",
     "shell.execute_reply": "2024-10-22T06:52:10.063616Z"
    },
    "papermill": {
     "duration": 7.537587,
     "end_time": "2024-10-22T06:52:10.066746",
     "exception": false,
     "start_time": "2024-10-22T06:52:02.529159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = fashion_mnist.load_data()\n",
    "        self.train_images = np.expand_dims(self.train_images.astype(np.float32)/255.0,axis=-1)\n",
    "        self.test_images = np.expand_dims(self.test_images.astype(np.float32)/255.0,axis=-1)\n",
    "        self.train_labels = self.train_labels.astype(np.int32)\n",
    "        self.test_labels = self.test_labels.astype(np.int32)\n",
    "        self.num_train, self.num_test = self.train_images.shape[0], self.test_images.shape[0]\n",
    "        \n",
    "    def get_batch_train(self, batch_size):\n",
    "        index = np.random.randint(0, np.shape(self.train_images)[0], batch_size)\n",
    "        #need to resize images to (224,224)\n",
    "        resized_images = tf.image.resize_with_pad(self.train_images[index],224,224,)\n",
    "        return resized_images.numpy(), self.train_labels[index]\n",
    "    \n",
    "    def get_batch_test(self, batch_size):\n",
    "        index = np.random.randint(0, np.shape(self.test_images)[0], batch_size)\n",
    "        #need to resize images to (224,224)\n",
    "        resized_images = tf.image.resize_with_pad(self.test_images[index],224,224,)\n",
    "        return resized_images.numpy(), self.test_labels[index]\n",
    "\n",
    "batch_size = 128\n",
    "dataLoader = DataLoader()\n",
    "x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n",
    "print(\"x_batch shape:\",x_batch.shape,\"y_batch shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9959fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:52:10.086921Z",
     "iopub.status.busy": "2024-10-22T06:52:10.086608Z",
     "iopub.status.idle": "2024-10-22T06:59:21.802638Z",
     "shell.execute_reply": "2024-10-22T06:59:21.801864Z"
    },
    "papermill": {
     "duration": 431.72813,
     "end_time": "2024-10-22T06:59:21.804518",
     "exception": false,
     "start_time": "2024-10-22T06:52:10.076388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nin():\n",
    "    import os\n",
    "    \n",
    "    weights_file_path = \"5.8_nin.weights.h5\"\n",
    "    if os.path.exists(weights_file_path):\n",
    "        net.load_weights(weights_file_path)\n",
    "    else:\n",
    "        print(f\"权重文件 {weights_file_path} 未找到，跳过加载权重\")\n",
    "    epoch = 5\n",
    "    num_iter = dataLoader.num_train//batch_size\n",
    "    for e in range(epoch):\n",
    "        for n in range(num_iter):\n",
    "            x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n",
    "            net.fit(x_batch, y_batch)\n",
    "            if n%20 == 0:\n",
    "                net.save_weights(\"5.8_nin.weights.h5\")\n",
    "                \n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.06, momentum=0.3, nesterov=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7)\n",
    "net.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_batch, y_batch = dataLoader.get_batch_train(batch_size)\n",
    "net.fit(x_batch, y_batch)\n",
    "train_nin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c4ced",
   "metadata": {
    "papermill": {
     "duration": 0.554472,
     "end_time": "2024-10-22T06:59:22.969121",
     "exception": false,
     "start_time": "2024-10-22T06:59:22.414649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "我们将训练好的参数读入，然后取测试数据计算测试准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b04fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:59:24.104265Z",
     "iopub.status.busy": "2024-10-22T06:59:24.103378Z",
     "iopub.status.idle": "2024-10-22T06:59:27.318518Z",
     "shell.execute_reply": "2024-10-22T06:59:27.317620Z"
    },
    "papermill": {
     "duration": 3.790867,
     "end_time": "2024-10-22T06:59:27.320541",
     "exception": false,
     "start_time": "2024-10-22T06:59:23.529674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net.load_weights(\"5.8_nin.weights.h5\")\n",
    "\n",
    "x_test, y_test = dataLoader.get_batch_test(2000)\n",
    "net.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157f798",
   "metadata": {
    "papermill": {
     "duration": 0.553906,
     "end_time": "2024-10-22T06:59:28.464727",
     "exception": false,
     "start_time": "2024-10-22T06:59:27.910821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 可视化展示模型最后一层的特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c7842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:59:29.613916Z",
     "iopub.status.busy": "2024-10-22T06:59:29.612963Z",
     "iopub.status.idle": "2024-10-22T06:59:35.896023Z",
     "shell.execute_reply": "2024-10-22T06:59:35.895065Z"
    },
    "papermill": {
     "duration": 6.883954,
     "end_time": "2024-10-22T06:59:35.900485",
     "exception": false,
     "start_time": "2024-10-22T06:59:29.016531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# 获取第一个 NiN 块的权重\n",
    "first_nin_block = net.layers[0]\n",
    "first_conv_layer = first_nin_block.layers[0]  # 获取 NiN 块中的第一个卷积层\n",
    "first_layer_weights = first_conv_layer.get_weights()[0]\n",
    "\n",
    "# 获取权重的形状\n",
    "weight_shape = first_layer_weights.shape\n",
    "print(\"第一层卷积核形状:\", weight_shape)\n",
    "\n",
    "# 创建一个函数来可视化权重\n",
    "def visualize_first_layer(weights):\n",
    "    # 假设权重形状为 (11, 11, 1, 96)，如果不是，可能需要调整\n",
    "    n_filters = weights.shape[3]\n",
    "    n_rows = int(np.ceil(np.sqrt(n_filters)))\n",
    "    n_cols = int(np.ceil(n_filters / n_rows))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(n_filters):\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # 获取单个滤波器的权重\n",
    "        w = weights[:, :, 0, i]\n",
    "        \n",
    "        # 标准化权重\n",
    "        w_normalized = stats.zscore(w.flatten()).reshape(w.shape)\n",
    "        \n",
    "        plt.imshow(w_normalized, cmap='viridis')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化第一层卷积核\n",
    "visualize_first_layer(first_layer_weights)\n",
    "\n",
    "# 打印权重的统计信息\n",
    "print(\"\\n原始权重:\")\n",
    "print(\"  最小值:\", np.min(first_layer_weights))\n",
    "print(\"  最大值:\", np.max(first_layer_weights))\n",
    "print(\"  平均值:\", np.mean(first_layer_weights))\n",
    "print(\"  标准差:\", np.std(first_layer_weights))\n",
    "\n",
    "# 计算标准化后的权重统计信息\n",
    "normalized_weights = stats.zscore(first_layer_weights.flatten()).reshape(first_layer_weights.shape)\n",
    "print(\"\\n标准化后的权重:\")\n",
    "print(\"  最小值:\", np.min(normalized_weights))\n",
    "print(\"  最大值:\", np.max(normalized_weights))\n",
    "print(\"  平均值:\", np.mean(normalized_weights))\n",
    "print(\"  标准差:\", np.std(normalized_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe3f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:59:37.016091Z",
     "iopub.status.busy": "2024-10-22T06:59:37.014864Z",
     "iopub.status.idle": "2024-10-22T06:59:40.598696Z",
     "shell.execute_reply": "2024-10-22T06:59:40.597541Z"
    },
    "papermill": {
     "duration": 4.140077,
     "end_time": "2024-10-22T06:59:40.600802",
     "exception": false,
     "start_time": "2024-10-22T06:59:36.460725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# 获取第二个 NiN 块的权重\n",
    "second_nin_block = net.layers[2]  # 假设第二个 NiN 块是网络的第三层（索引为2）\n",
    "second_conv_layer = second_nin_block.layers[0]  # 获取 NiN 块中的第一个卷积层\n",
    "second_layer_weights = second_conv_layer.get_weights()[0]\n",
    "\n",
    "# 获取权重的形状\n",
    "weight_shape = second_layer_weights.shape\n",
    "print(\"第二层卷积核形状:\", weight_shape)\n",
    "\n",
    "# 创建一个函数来可视化权重\n",
    "def visualize_second_layer(weights, num_filters_to_show=64):\n",
    "    n_filters = min(weights.shape[3], num_filters_to_show)\n",
    "    n_channels = weights.shape[2]\n",
    "    n_rows = int(np.ceil(np.sqrt(n_filters)))\n",
    "    n_cols = int(np.ceil(n_filters / n_rows))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(n_filters):\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # 对所有输入通道的权重取平均\n",
    "        w = np.mean(weights[:, :, :, i], axis=2)\n",
    "        \n",
    "        # 标准化权重\n",
    "        w_normalized = stats.zscore(w.flatten()).reshape(w.shape)\n",
    "        \n",
    "        plt.imshow(w_normalized, cmap='viridis')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化第二层卷积核\n",
    "visualize_second_layer(second_layer_weights)\n",
    "\n",
    "# 打印权重的统计信息\n",
    "print(\"\\n原始权重:\")\n",
    "print(\"  最小值:\", np.min(second_layer_weights))\n",
    "print(\"  最大值:\", np.max(second_layer_weights))\n",
    "print(\"  平均值:\", np.mean(second_layer_weights))\n",
    "print(\"  标准差:\", np.std(second_layer_weights))\n",
    "\n",
    "# 计算标准化后的权重统计信息\n",
    "normalized_weights = stats.zscore(second_layer_weights.flatten()).reshape(second_layer_weights.shape)\n",
    "print(\"\\n标准化后的权重:\")\n",
    "print(\"  最小值:\", np.min(normalized_weights))\n",
    "print(\"  最大值:\", np.max(normalized_weights))\n",
    "print(\"  平均值:\", np.mean(normalized_weights))\n",
    "print(\"  标准差:\", np.std(normalized_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e5902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:59:41.764654Z",
     "iopub.status.busy": "2024-10-22T06:59:41.764257Z",
     "iopub.status.idle": "2024-10-22T06:59:45.485927Z",
     "shell.execute_reply": "2024-10-22T06:59:45.484850Z"
    },
    "papermill": {
     "duration": 4.28555,
     "end_time": "2024-10-22T06:59:45.488161",
     "exception": false,
     "start_time": "2024-10-22T06:59:41.202611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# 获取第三个 NiN 块的权重\n",
    "third_nin_block = net.layers[4]  # 假设第三个 NiN 块是网络的第五层（索引为4）\n",
    "third_conv_layer = third_nin_block.layers[0]  # 获取 NiN 块中的第一个卷积层\n",
    "third_layer_weights = third_conv_layer.get_weights()[0]\n",
    "\n",
    "# 获取权重的形状\n",
    "weight_shape = third_layer_weights.shape\n",
    "print(\"第三层卷积核形状:\", weight_shape)\n",
    "\n",
    "# 创建一个函数来可视化权重\n",
    "def visualize_third_layer(weights, num_filters_to_show=64):\n",
    "    n_filters = min(weights.shape[3], num_filters_to_show)\n",
    "    n_rows = int(np.ceil(np.sqrt(n_filters)))\n",
    "    n_cols = int(np.ceil(n_filters / n_rows))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(n_filters):\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # 对所有输入通道的权重取平均\n",
    "        w = np.mean(weights[:, :, :, i], axis=2)\n",
    "        \n",
    "        # 标准化权重\n",
    "        w_normalized = stats.zscore(w.flatten()).reshape(w.shape)\n",
    "        \n",
    "        plt.imshow(w_normalized, cmap='viridis')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化第三层卷积核\n",
    "visualize_third_layer(third_layer_weights)\n",
    "\n",
    "# 打印权重的统计信息\n",
    "print(\"\\n原始权重:\")\n",
    "print(\"  最小值:\", np.min(third_layer_weights))\n",
    "print(\"  最大值:\", np.max(third_layer_weights))\n",
    "print(\"  平均值:\", np.mean(third_layer_weights))\n",
    "print(\"  标准差:\", np.std(third_layer_weights))\n",
    "\n",
    "# 计算标准化后的权重统计信息\n",
    "normalized_weights = stats.zscore(third_layer_weights.flatten()).reshape(third_layer_weights.shape)\n",
    "print(\"\\n标准化后的权重:\")\n",
    "print(\"  最小值:\", np.min(normalized_weights))\n",
    "print(\"  最大值:\", np.max(normalized_weights))\n",
    "print(\"  平均值:\", np.mean(normalized_weights))\n",
    "print(\"  标准差:\", np.std(normalized_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11af11",
   "metadata": {
    "papermill": {
     "duration": 0.559107,
     "end_time": "2024-10-22T06:59:46.606757",
     "exception": false,
     "start_time": "2024-10-22T06:59:46.047650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 小结\n",
    "\n",
    "* NiN重复使用由卷积层和代替全连接层的$1\\times 1$卷积层构成的NiN块来构建深层网络。\n",
    "* NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。\n",
    "* NiN的以上设计思想影响了后面一系列卷积神经网络的设计。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 492.494029,
   "end_time": "2024-10-22T06:59:50.010381",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-22T06:51:37.516352",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
